Phase,Tuning Option,Current Setting,Summary
# ═══════════════════════════════════════════════════════════════════════════════,,,
# PHASE 00 — IndexLab Harness & Event Stream,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Phase 00,RUNTIME_TRACE_ENABLED,true (env: RUNTIME_TRACE_ENABLED),[Backend + GUI: read-only] Master switch for runtime tracing. When enabled the runtime bridge writes fetch/LLM/parse events to NDJSON log and pushes them over WebSocket to the GUI. Disabling silences the entire event stream including all 17 GUI panels.
Phase 00,RUNTIME_TRACE_FETCH_RING,30 (env: RUNTIME_TRACE_FETCH_RING),[Backend only — no GUI control] Size of the circular buffer that holds the most recent fetch events in memory. The GUI Event Stream panel reads from this ring. Larger values keep more history in RAM; smaller values reduce memory footprint. Not exposed in any GUI panel.
Phase 00,RUNTIME_TRACE_LLM_RING,50 (env: RUNTIME_TRACE_LLM_RING),[Backend only — no GUI control] Size of the circular buffer for LLM call events. The LLM Output and LLM Metrics panels consume this ring. Controls how many recent LLM interactions are kept in memory for GUI display. Not exposed in any GUI panel.
Phase 00,RUNTIME_TRACE_LLM_PAYLOADS,true (env: RUNTIME_TRACE_LLM_PAYLOADS),[Backend only — no GUI control] When true the full LLM request/response payloads (prompt text + completion text) are captured in the trace ring and written to NDJSON. Useful for debugging but increases memory usage and log size significantly. Not exposed in any GUI panel.
Phase 00,RUNTIME_CONTROL_FILE,_runtime/control/runtime_overrides.json (env: RUNTIME_CONTROL_FILE),[Backend only — no GUI control] Path to the JSON file that holds runtime override values. The pipeline polls this file to pick up hot-reload config changes without restarting. Not exposed in any GUI panel.
Phase 00,EVENTS_JSON_WRITE,true (env: EVENTS_JSON_WRITE),[Backend only — no GUI control] Dual-write flag for events. When true events are written to both SQLite and JSON (NDJSON). This is the only dual-write flag that defaults to true — all others default to false. Turning it off would stop NDJSON event file creation.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# PHASE 01 — NeedSet Engine & Field State,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Phase 01,REQUIRED_WEIGHT — identity,5 (hardcoded in needsetEngine.js),[Hardcoded — no control] Weight multiplier for identity-level fields (brand/model/variant) in the NeedSet score formula. Identity fields get the highest multiplier because incorrect identity makes all other fields meaningless. Formula: need = missing_multiplier × conf_term × REQUIRED_WEIGHT × tier_deficit × min_refs_deficit × conflict_multiplier.
Phase 01,REQUIRED_WEIGHT — critical,4 (hardcoded in needsetEngine.js),[Hardcoded — no control] Weight multiplier for critical-level fields in the NeedSet formula. Critical fields are essential specs like sensor type or switch type that define the product's key characteristics.
Phase 01,REQUIRED_WEIGHT — required,2 (hardcoded in needsetEngine.js),[Hardcoded — no control] Weight multiplier for required-level fields in the NeedSet formula. Required fields are important specs that should be filled but are not identity-defining.
Phase 01,REQUIRED_WEIGHT — expected,1 (hardcoded in needsetEngine.js),[Hardcoded — no control] Weight multiplier for expected-level fields in the NeedSet formula. Expected fields are commonly available specs that are nice to have.
Phase 01,REQUIRED_WEIGHT — optional,1 (hardcoded in needsetEngine.js),[Hardcoded — no control] Weight multiplier for optional-level fields in the NeedSet formula. Optional fields have the lowest priority and get the base multiplier.
Phase 01,missing_multiplier,2.0 (hardcoded in needsetEngine.js),[Hardcoded — no control] NeedSet formula component. Applied when a field has no value (missing/unknown). Doubles the need score for empty fields to prioritize them in search. A higher value would make the engine more aggressive about filling empty fields.
Phase 01,tier_deficit_multiplier,2.0 when tier_preference includes 1 AND best_tier_seen > 1 (hardcoded),[Hardcoded — no control] NeedSet formula component. Kicks in when a field wants Tier 1 evidence but only has Tier 2+ evidence. Doubles the need score to push the engine toward manufacturer sources. Set to 1.0 when tier preference is satisfied.
Phase 01,min_refs_deficit_multiplier,1.5 when refs_found < min_refs (hardcoded),[Hardcoded — no control] NeedSet formula component. Applied when a field has fewer evidence references than its minimum policy requires. Increases need by 50% to drive additional source discovery. Set to 1.0 when min_refs is met.
Phase 01,conflict_multiplier,1.5 when conflict detected (hardcoded),[Hardcoded — no control] NeedSet formula component. Applied when multiple sources disagree on a field value. Increases need by 50% to drive tiebreaker evidence gathering. A higher value would make the engine more aggressive about resolving conflicts.
Phase 01,NEEDSET_EVIDENCE_DECAY_DAYS,14 (env: NEEDSET_EVIDENCE_DECAY_DAYS),"[Backend only — no GUI control] Half-life in days for evidence freshness decay. Evidence older than this many days has its effective confidence halved using exponential decay: effective_conf = conf × max(floor, 2^(-age/halfLife)). Controls how quickly old evidence loses influence on NeedSet scores."
Phase 01,NEEDSET_EVIDENCE_DECAY_FLOOR,0.30 (env: NEEDSET_EVIDENCE_DECAY_FLOOR),[Backend only — no GUI control] Minimum decay multiplier floor. Even very old evidence never drops below 30% of its original confidence. Prevents ancient but valid evidence from being completely ignored. Range: 0-1.
Phase 01,NEEDSET_CAP_IDENTITY_LOCKED,1.00 (env + GUI: Pipeline Settings slider 0.5-1.0),[GUI: Pipeline Settings → NeedSet Identity Caps → Locked] Maximum effective NeedSet score when identity is locked (confirmed). At 1.00 there is no cap — all fields can reach full need priority. Lowering this would artificially suppress need for fields when identity is confirmed.
Phase 01,NEEDSET_CAP_IDENTITY_PROVISIONAL,0.74 (env + GUI: Pipeline Settings slider 0.5-0.9),[GUI: Pipeline Settings → NeedSet Identity Caps → Provisional] Maximum NeedSet score when identity is provisional (70-95% confidence). Caps non-identity field priority to 0.74 until identity is fully confirmed. This prevents the engine from aggressively filling specs when it's not yet sure it has the right product.
Phase 01,NEEDSET_CAP_IDENTITY_CONFLICT,0.39 (env + GUI: Pipeline Settings slider 0.2-0.6),[GUI: Pipeline Settings → NeedSet Identity Caps → Conflict] Maximum NeedSet score when identity is in conflict state. Severely restricts non-identity field filling to 0.39 when multiple sources disagree about the product's identity. Forces the engine to resolve identity first before filling specs.
Phase 01,NEEDSET_CAP_IDENTITY_UNLOCKED,0.59 (env + GUI: Pipeline Settings slider 0.3-0.8),[GUI: Pipeline Settings → NeedSet Identity Caps → Unlocked] Maximum NeedSet score when identity has not been established at all. Moderately restricts field filling to 0.59 until at least some identity evidence arrives. Balances early-round exploration with identity-first priority.
Phase 01,Identity Lock Threshold,0.95 (hardcoded in needsetEngine.js),[Hardcoded — no control] Confidence threshold at which identity status transitions from provisional to locked. When identity confidence >= 0.95 and the gate is validated the status becomes 'locked' and all field caps are removed. Lowering this would lock identity earlier but risk false positives.
Phase 01,Identity Provisional Threshold,0.70 (hardcoded in needsetEngine.js),[Hardcoded — no control] Confidence threshold at which identity status transitions from unlocked to provisional. When identity confidence >= 0.70 the engine enters provisional mode allowing moderate field filling. Below 0.70 the engine is in 'unlocked' state with heavily restricted field access.
Phase 01,DEFAULT_IDENTITY_AUDIT_LIMIT,24 (hardcoded in needsetEngine.js),[Hardcoded — no control] Maximum number of rows shown in the identity audit context. Limits how many identity evidence rows are passed to downstream consumers for display/debugging. Increasing this shows more identity evidence in traces.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# PHASE 02 — SearchProfile & Alias Planning,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Phase 02,LLM_PLAN_DISCOVERY_QUERIES,false (env + GUI: Indexing → Planner LLM Enabled toggle),[GUI: Indexing Runtime → Planner LLM toggle] Master switch for the Phase 02 LLM search planner. When enabled the LLM generates additional search queries beyond deterministic aliases — including doc_hint_queries (spec/manual/pdf hints) and field_target_queries (queries targeting specific missing fields). When disabled only deterministic brand+model alias variants are used.
Phase 02,LLM_MODEL_PLAN,(env + GUI: Indexing → Planner Model dropdown),[GUI: Indexing Runtime → Planner Model dropdown] Selects which LLM model is used for search query planning. Dropdown is populated dynamically from available models. Prefers Gemini models for cost efficiency. Disabled when planner LLM is off.
Phase 02,LLM_MAX_OUTPUT_TOKENS_PLAN,1200 (env + GUI: Indexing → Planner Max Tokens dropdown),[GUI: Indexing Runtime → Planner Max Tokens dropdown] Maximum output tokens the planner LLM can generate per call. Controls how many search queries the LLM can propose in a single response. Uses preset dropdown values (256-8192). Capped to model maximum. Higher values allow more query suggestions but cost more.
Phase 02,LLM_PLAN_FALLBACK_MODEL,(env: LLM_PLAN_FALLBACK_MODEL),[Backend only — no GUI control] Fallback model used when the primary plan model fails or is unavailable. Part of the multi-provider failover chain. If empty no fallback is attempted and the planning call fails.
Phase 02,LLM_PLAN_PROVIDER,(env: LLM_PLAN_PROVIDER),[Backend only — no GUI control] Separate LLM provider for planning calls (e.g. 'gemini' vs 'openai'). Allows routing planning to a cheaper provider while using a more capable one for extraction. Inherits from LLM_PROVIDER if not set.
Phase 02,LLM_PLAN_BASE_URL,(env: LLM_PLAN_BASE_URL),[Backend only — no GUI control] Base URL for the planning LLM provider endpoint. Allows pointing planning at a different API server (e.g. a local model or different cloud endpoint). Inherits from LLM_BASE_URL if not set.
Phase 02,LLM_PLAN_API_KEY,(env: LLM_PLAN_API_KEY),[Backend only — no GUI control] API key for the planning LLM provider. Allows using a separate billing account for planning calls. Inherits from LLM_API_KEY if not set.
Phase 02,Deterministic Alias Cap,6 slugs (hardcoded in searchDiscovery.js buildModelSlugCandidates),[Hardcoded — no control] Maximum number of model slug candidates generated deterministically from the product name (spacing/hyphen/case variants). Limits explosion of alias combinations. The planner LLM can add more aliases on top of these.
Phase 02,LLM Alias Validation Cap,12 (hardcoded validation gate),[Hardcoded — no control] Maximum aliases the LLM planner is allowed to return. If the LLM response contains more than 12 aliases the extras are silently dropped during normalization. Prevents runaway alias generation.
Phase 02,LLM doc_hint_queries Cap,3 per hint (hardcoded validation gate),[Hardcoded — no control] Maximum queries per document hint category (spec/manual/pdf/datasheet). Prevents the LLM from generating too many redundant document-type queries for the same hint.
Phase 02,LLM field_target_queries Cap,3 per field (hardcoded validation gate),[Hardcoded — no control] Maximum queries targeting a single missing field. Prevents the LLM from over-investing search budget on a single hard-to-find field.
Phase 02,Dedupe Queries Cap,24 (hardcoded in queryBuilder.js),[Hardcoded — no control] Maximum total unique queries after deduplication. All query sources (deterministic aliases + LLM hints + field targets) are merged and deduped down to this cap. Controls the total search budget per round.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# PHASE 03 — Provider Search & SERP Triage,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Phase 03,SEARCH_PROVIDER,none (env + GUI: Indexing → Search Provider dropdown),[GUI: Indexing Runtime → Search Provider dropdown] Primary search provider for web queries. Options: none (disabled) / duckduckgo / searxng / bing / google / dual. 'dual' uses two providers for broader coverage. Setting to 'none' disables all web search — pipeline relies on seed URLs only.
Phase 03,SEARXNG_BASE_URL,empty string (env: SEARXNG_BASE_URL or SEARXNG_URL),[Backend only — no GUI control] URL of the self-hosted SearXNG metasearch instance. Required when search provider is 'searxng'. Typically http://localhost:8080. No GUI input field exists for this URL.
Phase 03,GOOGLE_CSE_KEY,(env: GOOGLE_CSE_KEY),[Backend only — no GUI control] Google Custom Search Engine API key. Required for Google CSE provider. Not exposed in GUI for security — set via environment variable only.
Phase 03,GOOGLE_CSE_CX,(env: GOOGLE_CSE_CX),[Backend only — no GUI control] Google Custom Search Engine ID. Identifies which custom search engine to query. Not exposed in GUI.
Phase 03,BING_SEARCH_KEY,(env: BING_SEARCH_KEY),[Backend only — no GUI control] Bing Search API key. Required when search provider is 'bing'. Not exposed in GUI for security.
Phase 03,BING_SEARCH_ENDPOINT,(env: BING_SEARCH_ENDPOINT),[Backend only — no GUI control] Bing Search API endpoint URL. Usually the default Azure endpoint. Not exposed in GUI.
Phase 03,DISABLE_GOOGLE_CSE,false (env: DISABLE_GOOGLE_CSE),[Backend only — no GUI control] Emergency kill switch to disable Google CSE entirely. Useful when CSE quota is exhausted. Does not appear in GUI.
Phase 03,CSE_RESCUE_ONLY_MODE,true (env: CSE_RESCUE_ONLY_MODE),[Backend only — no GUI control] When true Google CSE is only used as a rescue fallback (not primary search). CSE is expensive per query so this reserves it for situations where the primary provider fails. When false CSE is used alongside primary provider.
Phase 03,CSE_RESCUE_REQUIRED_ITERATION,2 (env: CSE_RESCUE_REQUIRED_ITERATION),[Backend only — no GUI control] Which iteration/round triggers CSE rescue activation. At iteration 2 if primary search yields insufficient results CSE is tried as backup. Lower values rescue earlier but use more CSE quota.
Phase 03,DUCKDUCKGO_ENABLED,true (env: DUCKDUCKGO_ENABLED),[Backend only — no GUI control] Enable DuckDuckGo as a search provider. DDG is free and requires no API key. Used as default search when no paid provider is configured.
Phase 03,DUCKDUCKGO_BASE_URL,https://html.duckduckgo.com/html/ (env: DUCKDUCKGO_BASE_URL),[Backend only — no GUI control] Base URL for DuckDuckGo HTML search. Uses the HTML endpoint for easy scraping. Not exposed in GUI.
Phase 03,DUCKDUCKGO_TIMEOUT_MS,8000 (env: DUCKDUCKGO_TIMEOUT_MS),[Backend only — no GUI control] Timeout in milliseconds for DuckDuckGo search requests. 8 seconds is the default. Increase if DDG is slow; decrease if you want faster timeouts.
Phase 03,LLM_SERP_RERANK_ENABLED,true (env + GUI: Indexing → Triage LLM Enabled toggle),[GUI: Indexing Runtime → Triage LLM toggle] Master switch for LLM-powered SERP reranking. When enabled the LLM scores and reranks SERP results after deterministic triage. When disabled only the rules-based deterministic reranker is used. Saves LLM costs but may miss nuanced relevance signals.
Phase 03,LLM_MODEL_TRIAGE,(env + GUI: Indexing → Triage Model dropdown),[GUI: Indexing Runtime → Triage Model dropdown] LLM model used for SERP reranking. Dropdown populated dynamically. Typically a fast/cheap model since triage is high-volume low-stakes.
Phase 03,LLM_MAX_OUTPUT_TOKENS_TRIAGE,(env + GUI: Indexing → Triage Max Tokens dropdown),[GUI: Indexing Runtime → Triage Max Tokens dropdown] Maximum output tokens for SERP triage LLM calls. Usually low (256-512) since triage responses are short score lists.
Phase 03,SERP_TRIAGE_ENABLED,true (env + GUI: Pipeline Settings → SERP Triage toggle),[GUI: Pipeline Settings → SERP Triage → Enabled toggle] Master switch for the entire SERP triage subsystem. When disabled all SERP results pass through unfiltered. When enabled URLs are scored and filtered by quality/relevance.
Phase 03,SERP_TRIAGE_MIN_SCORE,5 (env + GUI: Pipeline Settings slider 1-10),[GUI: Pipeline Settings → SERP Triage → Min Score slider] Minimum triage score a SERP result must achieve to be fetched. Scale 1-10 where higher is stricter. URLs scoring below this threshold are discarded. Lower values fetch more URLs (broader but noisier); higher values are more selective.
Phase 03,SERP_TRIAGE_MAX_URLS,12 (env + GUI: Pipeline Settings slider 5-30),[GUI: Pipeline Settings → SERP Triage → Max URLs slider] Maximum URLs to retain after triage scoring and filtering. Even if many URLs pass the min score threshold only the top N by score are kept. Controls fetch budget per search round.
Phase 03,SERP Identity Match Bonuses,strong: 2.0 / partial: 0.8 / weak: 0 / none: -1.5 (hardcoded in serpReranker.js),[Hardcoded — no control] SERP reranker identity match scoring. Strong identity match (brand+model+variant all present) gets +2.0 bonus. No identity match gets -1.5 penalty. These weights determine how aggressively the reranker favors URLs that mention the exact product.
Phase 03,SERP Brand Presence Bonus,2.5 (hardcoded in serpReranker.js),[Hardcoded — no control] Bonus points added to SERP score when the brand name appears in the URL title/snippet. Combined with model bonus for up to +5.0 total identity signal.
Phase 03,SERP Model Presence Bonus,2.5 (hardcoded in serpReranker.js),[Hardcoded — no control] Bonus points added to SERP score when the model name appears in the URL title/snippet.
Phase 03,SERP Spec/Manual Keyword Bonus,1.3 (hardcoded in serpReranker.js),[Hardcoded — no control] Bonus for URLs whose title/snippet contains spec/manual/datasheet keywords. Favors technical documentation over general content.
Phase 03,SERP Review/Benchmark Bonus,0.9 (hardcoded in serpReranker.js),[Hardcoded — no control] Bonus for URLs mentioning review/benchmark/test/comparison. Favors lab review content.
Phase 03,SERP Forum/Reddit Penalty,-0.9 (hardcoded in serpReranker.js),[Hardcoded — no control] Penalty for forum/reddit URLs. These sources are Tier 3-4 and often contain unreliable specs.
Phase 03,SERP Brand-in-Hostname Bonus,1.2 (hardcoded in serpReranker.js),[Hardcoded — no control] Bonus when the brand name appears in the hostname (e.g. razer.com for Razer products). Favors manufacturer sites (Tier 1).
Phase 03,SERP Wikipedia Penalty,-1.0 (hardcoded in serpReranker.js),[Hardcoded — no control] Penalty for Wikipedia URLs. Wikipedia rarely has detailed product specs and is not a primary source.
Phase 03,SERP Variant Guard Penalty,-3.0 (hardcoded in serpReranker.js),[Hardcoded — no control] Heavy penalty applied when the URL appears to be about a different variant of the product. Prevents fetching specs for the wrong product version.
Phase 03,SERP Multi-Model Hint Penalty,-1.5 (hardcoded in serpReranker.js),[Hardcoded — no control] Penalty when the URL appears to cover multiple products (comparison pages). These pages are harder to extract from accurately.
Phase 03,SERP Tier Bonuses,Tier 1: 1.5 / Tier 2: 0.5 / else: 0 (hardcoded in serpReranker.js),[Hardcoded — no control] Bonus points by evidence tier classification. Manufacturer/official sources (Tier 1) get +1.5; lab reviews (Tier 2) get +0.5. Tier 3-4 get no bonus.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# PHASE 04 — URL Health & Self-Healing,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Phase 04,FRONTIER_COOLDOWN_403_BASE,1800 seconds / 30 min (env + config.js),"[Backend only — no GUI control] Base cooldown in seconds after a 403 Forbidden response. Uses exponential backoff: actual_cooldown = base × 2^min(4, fetchCount-1). First 403 = 30min, second = 60min, third = 2hr, fourth = 4hr, max = 8hr. Prevents hammering domains that block the crawler."
Phase 04,FRONTIER_COOLDOWN_429_BASE,900 seconds / 15 min (env: FRONTIER_COOLDOWN_429_BASE),[Backend only — no GUI control] Base cooldown in seconds after a 429 Rate Limit response. Uses same exponential backoff as 403. Shorter base than 403 because rate limits are temporary. First 429 = 15min cooldown.
Phase 04,FRONTIER_COOLDOWN_404,259200 seconds / 72 hours (env: FRONTIER_COOLDOWN_404),[Backend only — no GUI control] Cooldown in seconds after a 404 Not Found. 3-day cooldown before retrying a missing URL. Long because 404s are usually permanent unless the page moves.
Phase 04,FRONTIER_COOLDOWN_404_REPEAT,1209600 seconds / 14 days (env: FRONTIER_COOLDOWN_404_REPEAT),[Backend only — no GUI control] Cooldown for repeated 404s on the same URL. 14-day cooldown after a URL has been 404 multiple times. Much longer to avoid wasting fetch budget on permanently dead URLs.
Phase 04,FRONTIER_COOLDOWN_410,7776000 seconds / 90 days (env: FRONTIER_COOLDOWN_410),[Backend only — no GUI control] Cooldown after a 410 Gone response. 90-day cooldown because 410 explicitly means the resource is permanently removed. Near-permanent suppression.
Phase 04,FRONTIER_COOLDOWN_TIMEOUT,21600 seconds / 6 hours (env: FRONTIER_COOLDOWN_TIMEOUT),[Backend only — no GUI control] Cooldown after a request timeout. 6-hour cooldown before retrying a URL that timed out. Shorter than error codes because timeouts can be transient.
Phase 04,FRONTIER_QUERY_COOLDOWN_SECONDS,21600 seconds / 6 hours (env: FRONTIER_QUERY_COOLDOWN_SECONDS),[Backend only — no GUI control] Cooldown between repeated search queries to the same domain. Prevents re-querying a domain too frequently within a single session.
Phase 04,FRONTIER_BLOCKED_DOMAIN_THRESHOLD,2 (env + config.js),[Backend only — no GUI control] Number of consecutive blocked/failed responses from a domain before the entire domain is suppressed. After 2 blocked outcomes the domain is temporarily blacklisted for the current run.
Phase 04,FRONTIER_REPAIR_SEARCH_ENABLED,true (env + config.js),[Backend only — no GUI control] Master switch for repair query emission. When a URL returns 404/410 the system generates a repair search query to find the content at a new URL on the same domain. Disabling stops all repair attempts.
Phase 04,FRONTIER_STRIP_TRACKING_PARAMS,true (env: FRONTIER_STRIP_TRACKING_PARAMS),[Backend only — no GUI control] Strip UTM and other tracking parameters from URLs before storing in the frontier DB. Prevents treating the same page with different tracking params as different URLs.
Phase 04,FRONTIER_ENABLE_SQLITE,true (env: FRONTIER_ENABLE_SQLITE),[Backend only — no GUI control] Use SQLite backend for frontier URL health tracking instead of JSON file. SQLite is faster for large URL inventories and supports concurrent access.
Phase 04,FRONTIER_DB_PATH,_intel/frontier/frontier.json (env: FRONTIER_DB_PATH),[Backend only — no GUI control] Path to the JSON-based frontier database file. Used as fallback when SQLite is disabled. Contains all URL health records with cooldown timestamps.
Phase 04,403 Backoff Max Exponent,4 (hardcoded in frontierSqlite.js),"[Hardcoded — no control] Maximum exponent in the exponential backoff formula for 403/429 cooldowns. Math.pow(2, min(4, fetchCount-1)) means max multiplier is 16× the base cooldown. Caps the maximum cooldown at ~8 hours for 403s and ~4 hours for 429s."
Phase 04,Path Penalty Not-Found Threshold,3 (hardcoded in frontierSqlite.js; env fallback 2),[Hardcoded — no control] Number of 404 outcomes on different paths of the same domain before applying a domain-wide path penalty. After 3 different paths return 404 the domain gets a penalty score increase.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# PHASE 05 — Parallel Fetch & Parse,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Phase 05,CONCURRENCY,2 (env + GUI: Indexing → Fetch Concurrency number input),"[GUI: Indexing Runtime → Fetch Concurrency input (min 1, max 64)] Maximum number of concurrent fetch operations. Controls how many URLs are being downloaded simultaneously. Higher values speed up fetching but increase risk of rate limiting and memory usage. Default of 2 is conservative."
Phase 05,PER_HOST_MIN_DELAY_MS,300 (env + GUI: Indexing → Per-Host Delay input),"[GUI: Indexing Runtime → Per-Host Delay ms input (min 0, max 120000, step 50)] Minimum delay in milliseconds between consecutive requests to the same host. Prevents overwhelming a single server. 300ms default = max ~3.3 req/s per host. Increase for polite crawling; decrease for faster fetching of tolerant hosts."
Phase 05,FETCH_SCHEDULER_ENABLED,false (env: FETCH_SCHEDULER_ENABLED),[Backend only — no GUI control] Feature flag for the new queue-based fetch scheduler (FetchScheduler + HostPacer + FallbackPolicy). When enabled fetches go through the scheduler with per-host pacing and dual-source fallback. When disabled the legacy direct fetch path is used.
Phase 05,FETCH_SCHEDULER_MAX_RETRIES,1 (env: FETCH_SCHEDULER_MAX_RETRIES),[Backend only — no GUI control] Maximum retry attempts per URL in the fetch scheduler. After 1 retry failure the URL is marked as failed and fallback policy decides next steps.
Phase 05,FETCH_SCHEDULER_FALLBACK_WAIT_MS,60000 (env: FETCH_SCHEDULER_FALLBACK_WAIT_MS),[Backend only — no GUI control] Wait time in milliseconds before attempting fallback fetch after primary failure. 60 seconds allows the target server to recover before retrying with alternate fetch method.
Phase 05,PREFER_HTTP_FETCHER,false (env: PREFER_HTTP_FETCHER),[Backend only — no GUI control] When true prefer plain HTTP fetch over Playwright/Crawlee dynamic rendering. Faster and lighter but misses JS-rendered content. Auto-set to true in 'fast' run profile.
Phase 05,DYNAMIC_CRAWLEE_ENABLED,true (env + GUI: Indexing → Crawlee Enabled toggle),[GUI: Indexing Runtime → Crawlee Enabled toggle] Master switch for Crawlee/Playwright-based dynamic page rendering. When disabled only plain HTTP fetch is available — JS-heavy pages will not render correctly. Essential for SPA sites and dynamically-loaded spec tables.
Phase 05,CRAWLEE_HEADLESS,true (env + GUI: Indexing → Crawlee Headless toggle),[GUI: Indexing Runtime → Crawlee Headless toggle] Run Playwright browser in headless mode (no visible window). Disable for debugging to watch the browser navigate. Headless is faster and uses less resources.
Phase 05,CRAWLEE_REQUEST_HANDLER_TIMEOUT_SECS,75 (env + GUI: Indexing → Handler Timeout input),"[GUI: Indexing Runtime → Handler Timeout (s) input (min 0, max 300)] Maximum time in seconds the Crawlee request handler waits before timing out a page fetch+parse operation. 75 seconds allows for slow-loading pages with lots of JS. Increase for very slow sites; decrease for faster failure."
Phase 05,DYNAMIC_FETCH_RETRY_BUDGET,2 (env + GUI: Indexing → Retry Budget input),"[GUI: Indexing Runtime → Retry Budget input (min 0, max 5)] Number of retry attempts for failed dynamic fetches per URL. Each retry uses exponential backoff. Higher values increase resilience but slow down failure detection."
Phase 05,DYNAMIC_FETCH_RETRY_BACKOFF_MS,1200 (env + GUI: Indexing → Retry Backoff input),"[GUI: Indexing Runtime → Retry Backoff (ms) input (min 0, max 30000, step 50)] Base backoff delay between dynamic fetch retries. Actual delay is backoff × attempt_number. 1200ms default provides reasonable spacing."
Phase 05,DYNAMIC_FETCH_POLICY_MAP_JSON,{} (env + GUI: Indexing → Domain Policy JSON textarea),"[GUI: Indexing Runtime → Domain Policy JSON textarea] Per-domain fetch policy overrides as JSON. Allows setting per-domain preferences like fetcher type, retry budget, and delay. Example: {""example.com"":{""prefer"":""playwright"",""retry_budget"":2,""per_host_delay_ms"":600}}. Advanced users can tune fetch behavior per site."
Phase 05,PAGE_GOTO_TIMEOUT_MS,15000 (env: PAGE_GOTO_TIMEOUT_MS),[Backend only — no GUI control] Playwright page.goto() timeout in milliseconds. How long to wait for initial page navigation before giving up. 15 seconds is generous for most pages.
Phase 05,PAGE_NETWORK_IDLE_TIMEOUT_MS,2000 (env: PAGE_NETWORK_IDLE_TIMEOUT_MS),[Backend only — no GUI control] Timeout for network idle detection after page load. Waits for the network to be quiet for this many ms before considering the page fully loaded. Important for SPA sites that make API calls after initial render.
Phase 05,POST_LOAD_WAIT_MS,0 (env: POST_LOAD_WAIT_MS),[Backend only — no GUI control] Additional wait time after page load and network idle before extracting content. 0 by default. Useful for pages with delayed animations or lazy-loaded content.
Phase 05,AUTO_SCROLL_ENABLED,false (env: AUTO_SCROLL_ENABLED),[Backend only — no GUI control] Enable automatic page scrolling to trigger lazy-loaded content. Some pages only load spec tables when scrolled into view. Disabled by default for speed.
Phase 05,AUTO_SCROLL_PASSES,0 (env: AUTO_SCROLL_PASSES),[Backend only — no GUI control] Number of scroll passes when auto-scroll is enabled. Each pass scrolls to bottom and back. More passes catch more lazy content but take longer.
Phase 05,AUTO_SCROLL_DELAY_MS,900 (env: AUTO_SCROLL_DELAY_MS),[Backend only — no GUI control] Delay between scroll passes in milliseconds. Allows time for content to load between scrolls.
Phase 05,GRAPHQL_REPLAY_ENABLED,true (env: GRAPHQL_REPLAY_ENABLED),[Backend only — no GUI control] Enable interception and replay of GraphQL queries observed during page load. Some sites load spec data via GraphQL — replaying these queries extracts structured data directly from the API.
Phase 05,MAX_GRAPHQL_REPLAYS,5 (env: MAX_GRAPHQL_REPLAYS),[Backend only — no GUI control] Maximum number of GraphQL queries to replay per page. Limits the API call budget to prevent excessive requests to the target site's GraphQL endpoint.
Phase 05,MAX_NETWORK_RESPONSES_PER_PAGE,1200 (env: MAX_NETWORK_RESPONSES_PER_PAGE),[Backend only — no GUI control] Maximum network responses to capture per page load. Limits memory usage from network interception on pages with many API calls. Responses beyond this limit are dropped.
Phase 05,ROBOTS_TXT_COMPLIANT,true (env: ROBOTS_TXT_COMPLIANT),[Backend only — no GUI control] Respect robots.txt directives. When true the fetcher checks robots.txt before crawling and skips disallowed paths. Disabling violates web etiquette but may access more content.
Phase 05,ROBOTS_TXT_TIMEOUT_MS,6000 (env: ROBOTS_TXT_TIMEOUT_MS),[Backend only — no GUI control] Timeout for fetching robots.txt files. If robots.txt doesn't respond within 6 seconds the URL is treated as allowed.
Phase 05,HostPacer DEFAULT_DELAY_MS,300 (hardcoded in hostPacer.js),[Hardcoded — no control] Default per-host pacing delay in the HostPacer concurrency module. Separate from PER_HOST_MIN_DELAY_MS — this is the default used internally by the FetchScheduler when no config override is provided.
Phase 05,FetchScheduler Default Concurrency,2 (hardcoded in fetchScheduler.js),[Hardcoded — no control] Default concurrency used internally by the FetchScheduler when no config override is provided. Separate from the CONCURRENCY config knob.
Phase 05,FetchScheduler Default Max Retries,1 (hardcoded in fetchScheduler.js),[Hardcoded — no control] Default max retries in the FetchScheduler internal logic.
Phase 05,FetchScheduler Retry Wait Intervals,1000ms / 60000ms (hardcoded in fetchScheduler.js),[Hardcoded — no control] Wait intervals in the FetchScheduler retry logic. 1000ms for immediate retry; 60000ms for backoff retry. Not configurable.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# PHASE 06A — Evidence Index Database,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Phase 06A,SPEC_DB_DIR,.specfactory_tmp (env: SPEC_DB_DIR),[Backend only — no GUI control] Directory where the SQLite spec database (including evidence index FTS5 tables) is stored. Controls where all persistent data lives on disk.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# PHASE 06B — Refetch Scheduler & Continuous Repair,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Phase 06B,AutomationQueue SQLite Tables,(created automatically in specDb),[Backend only — no GUI control] The AutomationQueue uses SQLite tables with state machine (queued→running→done/failed) and dedupe_key for job deduplication. No tuning controls exist for queue parameters — they are structural.
Phase 06B,Repair Dedupe Rule,1 repair job per (domain + brand/model + field_targets),[Hardcoded — no control] Deduplication rule for repair jobs in the automation queue. Only one repair job is allowed per unique combination of domain + product identity + target fields. Prevents duplicate repair attempts.
Phase 06B,Refresh TTL Window,Not implemented,[NOT IMPLEMENTED] Planned staleness refresh threshold per domain/doc_kind. Would control how old evidence can get before triggering a re-fetch. Currently no TTL policy exists.
Phase 06B,Per-Round Rediscovery Cap,Not implemented,[NOT IMPLEMENTED] Planned cap on how many rediscovery repair jobs can be enqueued per round. Would prevent runaway queue growth. Currently no cap exists.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# PHASE 07 — Tier Retrieval & Prime Sources,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Phase 07,RETRIEVAL_MAX_HITS_PER_FIELD,24 (env + GUI: Pipeline Settings slider 5-50),[GUI: Pipeline Settings → Retrieval → Max Hits Per Field slider] Maximum evidence hits returned per field from the retrieval engine. Controls how many candidate evidence snippets are considered for each spec field. Higher values provide more evidence for consensus but increase LLM costs.
Phase 07,RETRIEVAL_MAX_PRIME_SOURCES,8 (env + GUI: Pipeline Settings slider 3-20),[GUI: Pipeline Settings → Retrieval → Max Prime Sources slider] Maximum prime sources selected per field for LLM extraction. Controls how many top-ranked sources are passed to the extraction LLM. Higher values give the LLM more context but increase token costs.
Phase 07,RETRIEVAL_IDENTITY_FILTER_ENABLED,true (env + GUI: Pipeline Settings toggle),[GUI: Pipeline Settings → Retrieval → Identity Filter toggle] When enabled the retrieval engine filters out evidence snippets that fail identity gating — snippets that don't match the product's brand/model/variant. Prevents cross-product contamination in evidence.
Phase 07,DEFAULT_TIER_WEIGHTS,Tier1: 3 / Tier2: 2 / Tier3: 1 / Tier4: 0.65 / Tier5: 0.4 (hardcoded in tierAwareRetriever.js),[Hardcoded — no control] Base scoring weights for evidence by tier in the retrieval ranking formula. Tier 1 (manufacturer) sources are weighted 3× higher than Tier 3 (retail) sources. These weights are the foundation of the tier preference system.
Phase 07,DOC_KIND_WEIGHTS,manual_pdf: 1.5 / spec_pdf: 1.4 / support: 1.1 / spec_page: 1.0 / review: 0.95 / article: 0.85 / retail: 0.75 / forum: 0.6 / other: 0.55 (hardcoded),[Hardcoded — no control] Document type scoring weights in retrieval. PDFs (manuals/specs) are weighted highest because they contain the most reliable technical data. Forums and unknown documents get the lowest weights.
Phase 07,METHOD_WEIGHTS (retrieval),table: 1.25 / kv: 1.15 / json_ld: 1.1 / network_json: 1.1 / structured_meta: 1.05 / article: 0.95 / pdf: 0.95 / llm_extract: 0.85 / helper_supportive: 0.65 (hardcoded),"[Hardcoded — no control] Extraction method scoring weights in retrieval. Structured extraction methods (table parsing, key-value parsing) are weighted higher than LLM extraction because they are more deterministic and reliable."
Phase 07,Anchor Score Formula,"min(1.8, matches × 0.42) (hardcoded in tierAwareRetriever.js)",[Hardcoded — no control] Bonus score for evidence that contains field-specific anchor terms. Each anchor match adds 0.42 points up to a cap of 1.8 (so ~4 anchor matches reach the cap). Rewards evidence that uses the exact terminology expected for a field.
Phase 07,Identity Score Formula,"min(1.4, matches × 0.28) (hardcoded in tierAwareRetriever.js)",[Hardcoded — no control] Bonus score for evidence containing product identity tokens (brand/model/variant mentions). Each identity match adds 0.28 up to cap of 1.4 (5 matches reach cap). Rewards evidence that clearly references the correct product.
Phase 07,Unit Match Bonus,0.35 (hardcoded in tierAwareRetriever.js),"[Hardcoded — no control] Bonus when evidence contains the expected unit of measurement for the field (e.g. 'g' for weight, 'mm' for dimensions). Signals that the evidence likely contains a value in the correct unit."
Phase 07,Direct Field Match Bonus,0.65 (hardcoded in tierAwareRetriever.js),[Hardcoded — no control] Bonus when the evidence snippet directly mentions the field name/label. Strongest signal that the evidence is relevant to the target field.
Phase 07,Evidence Score Formula,tierWeight × 2.6 + docWeight × 1.5 + methodWeight × 0.85 + anchor + identity + unit + directField (hardcoded),"[Hardcoded — no control] Master formula combining all retrieval scoring signals. Tier weight is the dominant factor (2.6× multiplier), followed by document kind (1.5×) and extraction method (0.85×). Bonus scores are additive."
Phase 07,Evidence Pool Max Rows,"100-20000 range, default 4000 (hardcoded in tierAwareRetriever.js)",[Hardcoded — no control] Maximum rows in the evidence pool fed to retrieval. Clamped between 100 and 20000 with default 4000. Controls the size of the candidate pool before per-field filtering.
Phase 07,Snippets Per Source Cap,"8-300 range, default 120 (hardcoded in tierAwareRetriever.js)",[Hardcoded — no control] Maximum snippets retained per source URL. Prevents any single source from dominating the evidence pool. Default 120 allows deep coverage from rich sources.
Phase 07,Max Hits Cap Range,"1-80, default 24 (hardcoded in tierAwareRetriever.js)",[Hardcoded — no control] Internal max hits cap range with default 24. Applied when the config-driven RETRIEVAL_MAX_HITS_PER_FIELD is out of bounds.
Phase 07,Evidence Refs Limit,12 (hardcoded in tierAwareRetriever.js),[Hardcoded — no control] Maximum evidence reference IDs attached to each retrieval hit. Limits the cross-reference chain for audit trail purposes.
Phase 07,Reason Badges Limit,8 (hardcoded in tierAwareRetriever.js),[Hardcoded — no control] Maximum reason badges (explanation tags) attached to each retrieval hit for the GUI display.
Phase 07,Retrieval Anchors Limit,6 (hardcoded in tierAwareRetriever.js),[Hardcoded — no control] Maximum anchor terms used in retrieval query construction per field. Limits the query complexity.
Phase 07,Prime Sources Max Range,"1-20, default 8 (hardcoded in primeSourcesBuilder.js)",[Hardcoded — no control] Internal prime source cap range. Bounds-checks the config-driven RETRIEVAL_MAX_PRIME_SOURCES value.
Phase 07,Fallback Evidence Max Rows,"200-20000, default 6000 (hardcoded in primeSourcesBuilder.js)",[Hardcoded — no control] Maximum rows in the fallback evidence pool (used when provenance-based pool is insufficient). Larger than the primary pool to cast a wider net.
Phase 07,Provenance-Only Min Rows,24 (hardcoded in primeSourcesBuilder.js),[Hardcoded — no control] Minimum number of provenance-backed evidence rows before the retriever falls back to the broader evidence pool. If fewer than 24 provenance rows exist the fallback pool is merged in.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# PHASE 08 — Extraction Context Wiring,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Phase 08,LLM_MODEL_EXTRACT,(env + GUI: Indexing → Extract Role Model dropdown),[GUI: Indexing Runtime → Extract Role Model dropdown] LLM model used for the main spec value extraction. This is the workhorse model — it reads evidence snippets and outputs structured field values. Use a capable model for accuracy; use a fast model for speed.
Phase 08,LLM_MAX_OUTPUT_TOKENS_EXTRACT,1200 (env + GUI: Indexing → Extract Max Tokens dropdown),[GUI: Indexing Runtime → Extract Max Tokens dropdown] Maximum output tokens for extraction LLM calls. Controls how much structured data the LLM can output per extraction batch. 1200 is sufficient for most field batches.
Phase 08,LLM_EXTRACT_MAX_TOKENS,1200 (env: LLM_EXTRACT_MAX_TOKENS),[Backend only — no GUI control] Alternate token limit for extraction. May overlap with LLM_MAX_OUTPUT_TOKENS_EXTRACT depending on which code path is active.
Phase 08,LLM_EXTRACT_MAX_SNIPPETS_PER_BATCH,6 (env: LLM_EXTRACT_MAX_SNIPPETS_PER_BATCH),[Backend only — no GUI control] Maximum evidence snippets sent to the LLM per extraction batch. Controls the context window usage per call. More snippets give the LLM more evidence but increase input token costs. 6 is balanced.
Phase 08,LLM_EXTRACT_MAX_SNIPPET_CHARS,900 (env: LLM_EXTRACT_MAX_SNIPPET_CHARS),[Backend only — no GUI control] Maximum characters per snippet sent to the extraction LLM. Truncates long snippets to keep context window manageable. 900 chars preserves most spec table rows.
Phase 08,LLM_EXTRACT_SKIP_LOW_SIGNAL,true (env: LLM_EXTRACT_SKIP_LOW_SIGNAL),[Backend only — no GUI control] Skip evidence with low relevance signal before sending to LLM extraction. Saves tokens by not sending irrelevant content. When false all evidence is sent regardless of signal quality.
Phase 08,LLM_EXTRACT_REASONING_BUDGET,4096 (env: LLM_EXTRACT_REASONING_BUDGET),[Backend only — no GUI control] Token budget for reasoning/chain-of-thought in extraction LLM calls. Used with reasoning-capable models (DeepSeek Reasoner). Controls how much 'thinking' the model does before outputting values.
Phase 08,LLM_MODEL_VALIDATE,(env: LLM_MODEL_VALIDATE),[Backend only — no GUI control] LLM model for validation passes. Used to cross-check extracted values for accuracy. Can be a different model from extract for diversity.
Phase 08,LLM_MAX_OUTPUT_TOKENS_VALIDATE,1200 (env: LLM_MAX_OUTPUT_TOKENS_VALIDATE),[Backend only — no GUI control] Maximum output tokens for validation LLM calls.
Phase 08,LLM_MODEL_WRITE,(env: LLM_MODEL_WRITE),[Backend only — no GUI control] LLM model for summary writing. Used to generate human-readable product summaries from extracted spec data.
Phase 08,LLM_MAX_OUTPUT_TOKENS_WRITE,1200 (env: LLM_MAX_OUTPUT_TOKENS_WRITE),[Backend only — no GUI control] Maximum output tokens for summary writing LLM calls.
Phase 08,LLM_MODEL_FAST,(env + GUI: Indexing → Fast Pass Model dropdown),[GUI: Indexing Runtime → Fast Pass Model dropdown] Fast/cheap LLM model for low-stakes tasks like simple classification and triage. Should be the cheapest available model.
Phase 08,LLM_MAX_OUTPUT_TOKENS_FAST,(env + GUI: Indexing → Fast Pass Max Tokens dropdown),[GUI: Indexing Runtime → Fast Pass Max Tokens dropdown] Maximum output tokens for fast-pass LLM calls.
Phase 08,LLM_MODEL_REASONING,(env + GUI: Indexing → Reasoning Model dropdown),"[GUI: Indexing Runtime → Reasoning Model dropdown] LLM model for complex reasoning tasks (ambiguity resolution, conflict resolution). Should be the most capable available model."
Phase 08,LLM_MAX_OUTPUT_TOKENS_REASONING,32768 (env + GUI: Indexing → Reasoning Max Tokens dropdown),[GUI: Indexing Runtime → Reasoning Max Tokens dropdown] Maximum output tokens for reasoning LLM calls. Very high (32K) to allow extended chain-of-thought reasoning.
Phase 08,LLM_REASONING_MODE,auto-detected (env: LLM_REASONING_MODE),[Backend only — no GUI control] Enable chain-of-thought reasoning mode. Auto-detected to true if DeepSeek API key is set. Changes LLM prompt format to request explicit reasoning before answers.
Phase 08,LLM_REASONING_BUDGET,32768 (env: LLM_REASONING_BUDGET),[Backend only — no GUI control] General reasoning token budget (not extraction-specific). Controls the maximum reasoning tokens across all reasoning-mode calls.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# PHASE 08B — Visual Asset Capture,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Phase 08B,VISUAL_ASSET_CAPTURE_ENABLED,true (env: VISUAL_ASSET_CAPTURE_ENABLED),[Backend only — no GUI control] Master switch for visual asset capture (screenshots and images from source pages). When enabled Playwright captures spec table screenshots and product images during fetch. Disabling saves time and disk space but loses visual evidence.
Phase 08B,VISUAL_ASSET_CAPTURE_MAX_PER_SOURCE,5 (env: VISUAL_ASSET_CAPTURE_MAX_PER_SOURCE),[Backend only — no GUI control] Maximum visual assets (screenshots/images) to capture per source URL. Prevents excessive capture from image-heavy pages. 5 captures the most important spec tables and product images.
Phase 08B,VISUAL_ASSET_STORE_ORIGINAL,true (env: VISUAL_ASSET_STORE_ORIGINAL),[Backend only — no GUI control] Persist the original full-resolution image alongside derivatives. Useful for audit trails but increases storage. When false only derivatives (resized/compressed) are kept.
Phase 08B,VISUAL_ASSET_RETENTION_DAYS,30 (env: VISUAL_ASSET_RETENTION_DAYS),[Backend only — no GUI control] Days to retain visual assets before cleanup. After 30 days old assets are eligible for garbage collection.
Phase 08B,VISUAL_ASSET_PHASH_ENABLED,true (env: VISUAL_ASSET_PHASH_ENABLED),[Backend only — no GUI control] Enable perceptual hashing for near-duplicate image detection. When enabled captured images are hashed and compared — near-duplicates are clustered together to avoid redundant storage and processing.
Phase 08B,VISUAL_ASSET_REVIEW_FORMAT,webp (env: VISUAL_ASSET_REVIEW_FORMAT),"[Backend only — no GUI control] Image format for review derivatives. WebP offers good compression with quality. Alternatives: jpeg, png."
Phase 08B,VISUAL_ASSET_REVIEW_LG_MAX_SIDE,1600 (env: VISUAL_ASSET_REVIEW_LG_MAX_SIDE),[Backend only — no GUI control] Maximum dimension (width or height) for the large review derivative. Images are scaled down proportionally to fit within this bound. Used for detailed review in the GUI.
Phase 08B,VISUAL_ASSET_REVIEW_SM_MAX_SIDE,768 (env: VISUAL_ASSET_REVIEW_SM_MAX_SIDE),[Backend only — no GUI control] Maximum dimension for the small/thumbnail review derivative. Used for grid views and previews in the GUI.
Phase 08B,VISUAL_ASSET_REVIEW_LG_QUALITY,75 (env: VISUAL_ASSET_REVIEW_LG_QUALITY),[Backend only — no GUI control] Compression quality (0-100) for the large review derivative. Higher = better quality but larger file size.
Phase 08B,VISUAL_ASSET_REVIEW_SM_QUALITY,65 (env: VISUAL_ASSET_REVIEW_SM_QUALITY),[Backend only — no GUI control] Compression quality for the small review derivative. Lower than LG since thumbnails tolerate more compression.
Phase 08B,VISUAL_ASSET_REGION_CROP_MAX_SIDE,1024 (env: VISUAL_ASSET_REGION_CROP_MAX_SIDE),[Backend only — no GUI control] Maximum dimension for region-cropped derivatives. Region crops isolate specific spec tables or product images from full-page screenshots.
Phase 08B,VISUAL_ASSET_REGION_CROP_QUALITY,70 (env: VISUAL_ASSET_REGION_CROP_QUALITY),[Backend only — no GUI control] Compression quality for region crop derivatives.
Phase 08B,VISUAL_ASSET_LLM_MAX_BYTES,512000 (env: VISUAL_ASSET_LLM_MAX_BYTES),[Backend only — no GUI control] Hard cap in bytes per image sent to the LLM for vision analysis. 500KB prevents excessive token usage from large images. Images exceeding this are further compressed or rejected.
Phase 08B,VISUAL_ASSET_MIN_WIDTH,320 (env: VISUAL_ASSET_MIN_WIDTH),[Backend only — no GUI control] Minimum image width in pixels for the quality gate. Images narrower than 320px are rejected as too small to contain useful spec data.
Phase 08B,VISUAL_ASSET_MIN_HEIGHT,320 (env: VISUAL_ASSET_MIN_HEIGHT),[Backend only — no GUI control] Minimum image height in pixels for the quality gate.
Phase 08B,VISUAL_ASSET_MIN_SHARPNESS,80 (env: VISUAL_ASSET_MIN_SHARPNESS),[Backend only — no GUI control] Minimum sharpness score (Laplacian variance) for the quality gate. Images below this threshold are rejected as too blurry to extract text from. Higher values are stricter.
Phase 08B,VISUAL_ASSET_MIN_ENTROPY,2.5 (env: VISUAL_ASSET_MIN_ENTROPY),[Backend only — no GUI control] Minimum entropy threshold for the quality gate. Rejects blank/solid-color images that contain no useful information. Entropy measures information density.
Phase 08B,VISUAL_ASSET_MAX_PHASH_DISTANCE,10 (env: VISUAL_ASSET_MAX_PHASH_DISTANCE),[Backend only — no GUI control] Maximum perceptual hash distance for near-duplicate clustering. Images with pHash distance <= 10 are considered near-duplicates. Lower values are stricter (fewer duplicates detected).
Phase 08B,VISUAL_ASSET_HERO_SELECTOR_MAP_JSON,(env: VISUAL_ASSET_HERO_SELECTOR_MAP_JSON),[Backend only — no GUI control] Per-domain CSS selector overrides for hero image/region detection as JSON. Allows targeting specific page regions for visual capture on known sites.
Phase 08B,CAPTURE_PAGE_SCREENSHOT_ENABLED,true (env: CAPTURE_PAGE_SCREENSHOT_ENABLED),[Backend only — no GUI control] Enable full-page screenshot capture during fetch. Used for visual evidence and debugging. Different from VISUAL_ASSET_CAPTURE which targets specific elements.
Phase 08B,CAPTURE_PAGE_SCREENSHOT_FORMAT,jpeg (env: CAPTURE_PAGE_SCREENSHOT_FORMAT),[Backend only — no GUI control] Format for full-page screenshots: jpeg or png. JPEG is smaller; PNG is lossless.
Phase 08B,CAPTURE_PAGE_SCREENSHOT_QUALITY,62 (env: CAPTURE_PAGE_SCREENSHOT_QUALITY),[Backend only — no GUI control] JPEG quality for full-page screenshots (0-100). 62 is a good balance of quality and file size.
Phase 08B,CAPTURE_PAGE_SCREENSHOT_MAX_BYTES,2200000 (env: CAPTURE_PAGE_SCREENSHOT_MAX_BYTES),[Backend only — no GUI control] Maximum file size for page screenshots in bytes (~2.2MB). Larger screenshots are rejected to prevent storage bloat.
Phase 08B,CAPTURE_PAGE_SCREENSHOT_SELECTORS,"table,[data-spec-table],.specs-table,.spec-table,.specifications (env)",[Backend only — no GUI control] CSS selectors for targeting spec table elements in screenshots. When a page contains these selectors the screenshot focuses on the spec table area rather than the full page.
Phase 08B,RUNTIME_CAPTURE_SCREENSHOTS,false (env: RUNTIME_CAPTURE_SCREENSHOTS),[Backend only — no GUI control] Master runtime switch for screenshot capture. Different from CAPTURE_PAGE_SCREENSHOT_ENABLED — this controls whether the runtime bridge emits screenshot events.
Phase 08B,RUNTIME_SCREENSHOT_MODE,last_only (env: RUNTIME_SCREENSHOT_MODE),[Backend only — no GUI control] Screenshot mode: 'last_only' keeps only the most recent screenshot per source (saves disk); 'all' keeps every screenshot taken during the run.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# PHASE 09 — Convergence Loop & Stop Conditions,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Phase 09,CONVERGENCE_MAX_ROUNDS,3 (env + GUI: Pipeline Settings slider 1-12),[GUI: Pipeline Settings → Convergence Loop → Max Rounds slider] Maximum number of convergence rounds before the engine stops. Each round performs a full search→fetch→parse→extract→score cycle. More rounds find more evidence but cost more time and LLM tokens. Default 3 is conservative.
Phase 09,CONVERGENCE_NO_PROGRESS_LIMIT,2 (env + GUI: Pipeline Settings slider 1-6),[GUI: Pipeline Settings → Convergence Loop → No-Progress Streak slider] Number of consecutive rounds with no new evidence before stopping. If 2 rounds in a row yield no new field values the engine gives up. Lower values stop sooner; higher values are more persistent.
Phase 09,CONVERGENCE_MAX_LOW_QUALITY_ROUNDS,1 (env + GUI: Pipeline Settings slider 1-6),[GUI: Pipeline Settings → Convergence Loop → Max Low Quality Rounds slider] Maximum rounds where average confidence remains below the low-quality threshold before stopping. Default 1 means one bad round triggers early stop.
Phase 09,CONVERGENCE_LOW_QUALITY_CONFIDENCE,0.20 (env + GUI: Pipeline Settings slider 0-1),[GUI: Pipeline Settings → Convergence Loop → Low Quality Confidence slider (step 0.05)] Confidence threshold below which a round is considered 'low quality'. If average confidence across all extracted fields is below 0.20 the round is flagged. Combined with MAX_LOW_QUALITY_ROUNDS to trigger early stop.
Phase 09,CONVERGENCE_IDENTITY_FAIL_FAST_ROUNDS,1 (env: CONVERGENCE_IDENTITY_FAIL_FAST_ROUNDS),[Backend only — no GUI control] Number of rounds after which identity failure triggers fast-fail stop. After 1 round if identity gate still fails (can't confirm product identity) the engine stops immediately. Prevents wasting budget extracting specs for the wrong product.
Phase 09,CONVERGENCE_MAX_DISPATCH_QUERIES,20 (env + GUI: Pipeline Settings slider 5-50),[GUI: Pipeline Settings → Convergence Loop → Max Dispatch Queries slider] Maximum search queries dispatched per convergence round. Controls the search budget per round. Higher values cast a wider search net but use more search provider quota.
Phase 09,CONVERGENCE_MAX_TARGET_FIELDS,30 (env + GUI: Pipeline Settings slider 5-80),[GUI: Pipeline Settings → Convergence Loop → Max Target Fields slider] Maximum fields targeted per convergence round. The NeedSet engine selects the top N fields by need score for each round. Higher values attempt more fields per round but dilute focus.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# PHASE 10 — Learning & Compounding,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Phase 10,Learning Confidence Threshold,0.85 (hardcoded in learningUpdater.js),[Hardcoded — no control] Minimum confidence threshold for accepting a field value into learning stores. Values below 0.85 confidence are not learned — they might be wrong. Higher values are more conservative; lower values learn more aggressively.
Phase 10,Component Lexicon Decay Days,90 (hardcoded in learningStores.js),"[Hardcoded — no control] Half-life in days for component lexicon entries. Learned component names (e.g. sensor models, switch types) lose 50% of their weight every 90 days. Ensures the learning system adapts as product lines change."
Phase 10,Component Lexicon Expire Days,180 (hardcoded in learningStores.js),[Hardcoded — no control] Expiration in days for component lexicon entries. After 180 days unused component names are purged entirely from the learning store.
Phase 10,Field Anchors Decay Days,60 (hardcoded in learningStores.js),[Hardcoded — no control] Half-life in days for learned field anchor phrases. Anchor terms (phrases that reliably indicate a field value nearby) decay with 60-day half-life. Shorter than component lexicon because language patterns change faster.
Phase 10,URL Memory Decay Days,120 (hardcoded in learningStores.js),[Hardcoded — no control] Half-life in days for URL memory entries. Learned canonical URLs and their quality assessments decay over 120 days. Balances remembering good sources with adapting to site changes.
Phase 10,HELPER_FILES_ENABLED,true (env: HELPER_FILES_ENABLED),[Backend only — no GUI control] Master switch for helper file system (pre-existing knowledge base). When enabled the system loads known values and field hints from helper files to bootstrap extraction.
Phase 10,HELPER_FILES_ROOT,helper_files (env: HELPER_FILES_ROOT),"[Backend only — no GUI control] Root directory for helper files. Contains per-category knowledge bases with known values, field rules, and extraction hints."
Phase 10,HELPER_SUPPORTIVE_ENABLED,true (env: HELPER_SUPPORTIVE_ENABLED),[Backend only — no GUI control] Enable supportive helper mode where helper files provide seed evidence. Helpers act as low-confidence Tier 4 sources that can be confirmed or overridden by web evidence.
Phase 10,HELPER_SUPPORTIVE_FILL_MISSING,true (env: HELPER_SUPPORTIVE_FILL_MISSING),[Backend only — no GUI control] Allow helpers to fill in missing field values when no web evidence exists. Acts as a last resort to provide values for hard-to-find fields.
Phase 10,HELPER_SUPPORTIVE_MAX_SOURCES,6 (env: HELPER_SUPPORTIVE_MAX_SOURCES),[Backend only — no GUI control] Maximum helper-derived sources considered per field. Limits the influence of pre-existing knowledge vs. fresh web evidence.
Phase 10,HELPER_AUTO_SEED_TARGETS,true (env: HELPER_AUTO_SEED_TARGETS),[Backend only — no GUI control] Automatically use helper files to seed target URLs for known product pages. Provides starting URLs for products where the manufacturer page is already known.
Phase 10,FIELD_REWARD_HALF_LIFE_DAYS,45 (env: FIELD_REWARD_HALF_LIFE_DAYS),[Backend only — no GUI control] Half-life in days for field reward decay. The bandit batch strategy uses reward scores to prioritize field extraction order. Older rewards decay to adapt to changing extraction difficulty.
Phase 10,BATCH_STRATEGY,bandit (env: BATCH_STRATEGY),[Backend only — no GUI control] Strategy for ordering field extraction batches. 'bandit' uses multi-armed bandit with reward decay to prioritize fields most likely to yield values. Alternative strategies may exist but bandit is the default.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# PHASE 11 — Worker Lanes & Runtime Control,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Phase 11,WORKERS_SEARCH,Not implemented (planned env: WORKERS_SEARCH),[NOT IMPLEMENTED] Planned concurrent worker count for the search lane. Would control how many parallel search queries execute simultaneously. Currently search runs inline.
Phase 11,WORKERS_FETCH,Not implemented (planned env: WORKERS_FETCH),[NOT IMPLEMENTED] Planned concurrent worker count for the fetch lane. Would control parallel page downloads. Currently controlled by CONCURRENCY knob.
Phase 11,WORKERS_PARSE,Not implemented (planned env: WORKERS_PARSE),[NOT IMPLEMENTED] Planned concurrent worker count for the parse lane. Would control parallel HTML/PDF parsing. Currently parsing runs inline after fetch.
Phase 11,WORKERS_LLM,Not implemented (planned env: WORKERS_LLM),[NOT IMPLEMENTED] Planned concurrent worker count for the LLM lane. Would control parallel LLM extraction calls. Currently limited by LLM_MAX_CALLS_PER_ROUND.
Phase 11,WORKER_HEALTH_CHECK_INTERVAL_MS,Not implemented,[NOT IMPLEMENTED] Planned health check frequency for worker processes. Would monitor and restart unhealthy workers.
Phase 11,WORKER_RESTART_BACKOFF_MS,Not implemented,[NOT IMPLEMENTED] Planned backoff before restarting a failed worker. Would prevent tight restart loops.
Phase 11,429_BLOCK_RATE_THRESHOLD,Not implemented,[NOT IMPLEMENTED] Planned threshold for maximum 429/blocked response rate before load shedding. Would automatically reduce concurrency when too many rate limits are hit.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# PHASE 12 — Multi-Product Batch Automation,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Phase 12,MAX_BATCH_SIZE_CONFIRMATION,200 (planned),[NOT IMPLEMENTED] Planned threshold above which batch runs require user confirmation. Prevents accidentally queuing hundreds of products.
Phase 12,MAX_PARALLEL_PRODUCT_WORKERS,Not implemented,[NOT IMPLEMENTED] Planned maximum concurrent product runs in batch mode. Would balance throughput vs. system resource usage.
Phase 12,MAX_RUN_SECONDS_PER_PRODUCT,300 (env: MAX_RUN_SECONDS),[Backend only — no GUI control] Maximum runtime in seconds per individual product run. After 300 seconds (5 minutes) the run is terminated. Prevents runaway runs from blocking the queue.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# CONSENSUS SCORING ENGINE,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Consensus,CONSENSUS_LLM_WEIGHT_TIER1,0.60 (env + GUI: Pipeline Settings slider 0.3-0.9),[GUI: Pipeline Settings → Consensus → LLM Tier 1 slider] Weight applied to LLM-extracted values from Tier 1 (manufacturer) sources in consensus scoring. 0.60 means LLM extractions from manufacturer pages contribute 60% of their raw weight. Higher values trust LLM extraction more; lower values rely more on deterministic extraction.
Consensus,CONSENSUS_LLM_WEIGHT_TIER2,0.40 (env + GUI: Pipeline Settings slider 0.2-0.7),[GUI: Pipeline Settings → Consensus → LLM Tier 2 slider] Weight for LLM-extracted values from Tier 2 (lab review) sources. Lower than Tier 1 because lab reviews may describe test results rather than official specs.
Consensus,CONSENSUS_LLM_WEIGHT_TIER3,0.20 (env + GUI: Pipeline Settings slider 0.1-0.4),[GUI: Pipeline Settings → Consensus → LLM Tier 3 slider] Weight for LLM-extracted values from Tier 3 (retail) sources. Retail listings often have inaccurate specs so LLM extractions are downweighted.
Consensus,CONSENSUS_LLM_WEIGHT_TIER4,0.15 (env + GUI: Pipeline Settings slider 0.05-0.3),[GUI: Pipeline Settings → Consensus → LLM Tier 4 slider] Weight for LLM-extracted values from Tier 4 (unverified) sources. Lowest weight — LLM values from unknown sources are treated with maximum skepticism.
Consensus,CONSENSUS_TIER1_WEIGHT,1.00 (env + GUI: Pipeline Settings slider 0.8-1.0),[GUI: Pipeline Settings → Consensus → Tier 1 Weight slider] Overall weight multiplier for all evidence from Tier 1 sources in consensus voting. 1.00 means full weight. Manufacturer data is treated as ground truth.
Consensus,CONSENSUS_TIER2_WEIGHT,0.80 (env + GUI: Pipeline Settings slider 0.5-0.9),[GUI: Pipeline Settings → Consensus → Tier 2 Weight slider] Overall weight for Tier 2 evidence. Lab reviews get 80% weight relative to manufacturer sources.
Consensus,CONSENSUS_TIER3_WEIGHT,0.45 (env + GUI: Pipeline Settings slider 0.2-0.6),[GUI: Pipeline Settings → Consensus → Tier 3 Weight slider] Overall weight for Tier 3 evidence. Retail listings get 45% weight — less than half the weight of manufacturer data.
Consensus,CONSENSUS_TIER4_WEIGHT,0.25 (env + GUI: Pipeline Settings slider 0.1-0.4),[GUI: Pipeline Settings → Consensus → Tier 4 Weight slider] Overall weight for Tier 4 evidence. Unverified sources get 25% weight — heavily downweighted but still contribute to consensus.
Consensus,METHOD_WEIGHT — network_json,1.0 (hardcoded in consensusEngine.js),[Hardcoded — no control] Consensus weight for values extracted via network JSON interception. Highest weight because network JSON is structured data directly from the site's API — most reliable extraction method.
Consensus,METHOD_WEIGHT — adapter_api,0.95 (hardcoded in consensusEngine.js),[Hardcoded — no control] Consensus weight for values from adapter APIs (structured importers like Supabase/manufacturer APIs). Nearly as reliable as network JSON.
Consensus,METHOD_WEIGHT — structured_meta,0.90 (hardcoded in consensusEngine.js),"[Hardcoded — no control] Consensus weight for structured metadata extraction (JSON-LD, microdata, OpenGraph). Highly reliable for supported schemas."
Consensus,METHOD_WEIGHT — pdf,0.82 (hardcoded in consensusEngine.js),[Hardcoded — no control] Consensus weight for PDF-extracted values. PDFs are authoritative but extraction can miss nuances in layout.
Consensus,METHOD_WEIGHT — table_kv,0.78 (hardcoded in consensusEngine.js),[Hardcoded — no control] Consensus weight for key-value pairs extracted from spec tables. Very reliable for structured spec pages.
Consensus,METHOD_WEIGHT — dom,0.4 (hardcoded in consensusEngine.js),[Hardcoded — no control] Consensus weight for raw DOM extraction. Lower reliability due to unstructured page content and potential noise.
Consensus,METHOD_WEIGHT — llm_extract,0.2 (hardcoded in consensusEngine.js),[Hardcoded — no control] Consensus weight for LLM-extracted values. Lowest method weight because LLM extraction can hallucinate. The LLM tier weights (above) further modify this base weight by source tier.
Consensus,POLICY_BONUS,0.3 (hardcoded in consensusEngine.js),[Hardcoded — no control] Tiebreaker bonus in consensus scoring for the value that matches the selection policy. When two values have similar scores the policy-preferred one gets +0.3.
Consensus,Weighted Majority Threshold,1.1× (hardcoded in consensusEngine.js),[Hardcoded — no control] The winning value must score at least 1.1× the second-best value to be accepted as majority. Prevents accepting values with razor-thin margins.
Consensus,Strict Acceptance Domain Count,3 (hardcoded in consensusEngine.js),[Hardcoded — no control] Minimum number of unique approved domains that must agree on a value for strict acceptance. Prevents accepting values confirmed by only 1-2 sources.
Consensus,Relaxed Acceptance Domain Count,2 (hardcoded in consensusEngine.js),[Hardcoded — no control] Domain count threshold for relaxed acceptance (when allowBelowPassTargetFill is enabled). Allows accepting values with fewer confirming domains.
Consensus,Instrumented Field Threshold,3 (hardcoded in consensusEngine.js),[Hardcoded — no control] Minimum instrumented extraction count for acceptance of instrumented fields. Requires 3 or more instrumented extractions to confirm.
Consensus,Confidence Scoring Base,0.7 when domains >= 3 (hardcoded in consensusEngine.js),"[Hardcoded — no control] Base confidence score formula: 0.7 if 3+ domains agree, otherwise domain_count/4. Additional bonuses: +0.2 for Tier 1 match, +0.1 for multiple sources, +0.1 for no conflicts. Max confidence is ~1.0."
Consensus,Pass Target — Identity/Strong Fields,5 (hardcoded in consensusEngine.js),[Hardcoded — no control] Pass target for commonly-wrong fields (fields known to frequently have wrong values). These fields need 5 confirming domains to pass quality gate.
Consensus,Pass Target — Normal Fields,3 (hardcoded in consensusEngine.js),[Hardcoded — no control] Default pass target for normal fields. 3 confirming domains needed. Fields in PASS_EXEMPT list have target 0.
Consensus,ALLOW_BELOW_PASS_TARGET_FILL,false (env: ALLOW_BELOW_PASS_TARGET_FILL),[Backend only — no GUI control] When true allows accepting values that haven't met the pass target domain count. Relaxes consensus requirements — useful for rare products with few sources.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# IDENTITY GATE,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Identity Gate,IDENTITY_GATE_PUBLISH_THRESHOLD,0.70 (env: IDENTITY_GATE_PUBLISH_THRESHOLD),[Backend only — no GUI control] Minimum identity confidence required to publish/accept extracted spec values. Below 0.70 identity confidence no values are published — the system isn't confident enough that it has the right product.
Identity Gate,Base Match Threshold,0.80 (hardcoded in identityGate.js),[Hardcoded — no control] Base identity match threshold before ambiguity adjustments. A source must match product identity tokens at >= 80% to pass the identity gate. Adjusted up/down by ambiguity level.
Identity Gate,Easy Ambiguity Reduction,-0.15 (hardcoded in identityGate.js),[Hardcoded — no control] Threshold reduction for easy ambiguity (1 sibling product). Lowers match requirement to 0.65 because there's little confusion risk with few similar products.
Identity Gate,Medium Ambiguity Reduction,-0.10 (hardcoded in identityGate.js),[Hardcoded — no control] Threshold reduction for medium ambiguity (2-3 siblings). Lowers to 0.70.
Identity Gate,Hard Ambiguity Reduction,-0.02 (hardcoded in identityGate.js),[Hardcoded — no control] Threshold reduction for hard ambiguity (4-5 siblings). Nearly no reduction — 0.78 threshold. Many similar products require strict matching.
Identity Gate,Very Hard Ambiguity Increase,+0.01 (hardcoded in identityGate.js),[Hardcoded — no control] Threshold increase for very hard ambiguity (6-8 siblings). Raises to 0.81. More siblings = stricter matching needed.
Identity Gate,Extra Hard Ambiguity Increase,+0.03 (hardcoded in identityGate.js),[Hardcoded — no control] Threshold increase for extra hard ambiguity (9+ siblings). Raises to 0.83. Maximum strictness for product families with many variants.
Identity Gate,Missing Strong ID Penalty,-0.05 (hardcoded in identityGate.js),[Hardcoded — no control] Threshold reduction when strong identity signals are missing. Loosens the gate slightly when the product lacks distinctive identifiers.
Identity Gate,Hard + Missing ID Increase,+0.03 (hardcoded in identityGate.js),[Hardcoded — no control] Additional threshold increase when hard ambiguity AND missing strong ID combine. Counter-intuitively tightens the gate because ambiguous products without strong IDs are the riskiest.
Identity Gate,Very Hard + Missing ID Increase,+0.05 (hardcoded in identityGate.js),[Hardcoded — no control] Additional threshold increase for very hard ambiguity with missing strong ID.
Identity Gate,Extra Hard + Missing ID Increase,+0.08 (hardcoded in identityGate.js),[Hardcoded — no control] Additional threshold increase for extra hard ambiguity with missing strong ID. Maximum penalty scenario.
Identity Gate,Threshold Bounds,"[0.62, 0.92] (hardcoded in identityGate.js)",[Hardcoded — no control] Hard floor and ceiling for the identity match threshold after all adjustments. No matter what the ambiguity the threshold never goes below 0.62 or above 0.92.
Identity Gate,Numeric Token Boost,0.10 (hardcoded in identityGate.js),[Hardcoded — no control] Small bonus applied to identity match score when numeric model tokens are matched. Helps identify products like 'AW610M' where the numeric portion is distinctive.
Identity Gate,Numeric Range Threshold,3 (hardcoded in identityGate.js),[Hardcoded — no control] Maximum allowed spread between numeric tokens in identity matching. If the range between numbers exceeds 3 the match is flagged as potentially wrong (e.g. model 600 vs model 610).
Identity Gate,Quality Gate Identity Threshold,0.70 (hardcoded in qualityGate.js),[Hardcoded — no control] Identity confidence threshold used in the quality gate. Matches IDENTITY_GATE_PUBLISH_THRESHOLD. Evidence below this confidence is flagged in quality reports.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# PARSING — Static DOM (01),,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Parsing 01,STATIC_DOM_EXTRACTOR_ENABLED,true (env: STATIC_DOM_EXTRACTOR_ENABLED),[Backend only — no GUI control] Master switch for the static DOM extractor. When enabled Cheerio/regex parses HTML for spec data without running JavaScript. Fast but misses dynamically-loaded content.
Parsing 01,STATIC_DOM_MODE,cheerio (env: STATIC_DOM_MODE),[Backend only — no GUI control] DOM parser mode. 'cheerio' uses the Cheerio library for fast jQuery-like parsing. 'regex_fallback' uses regular expressions when Cheerio fails to parse malformed HTML.
Parsing 01,STATIC_DOM_TARGET_MATCH_THRESHOLD,0.55 (env: STATIC_DOM_TARGET_MATCH_THRESHOLD),[Backend only — no GUI control] Threshold (0-1) for fuzzy matching field names in DOM elements. At 0.55 a DOM label must be 55% similar to a known field name to be considered a match. Lower values catch more variations but risk false positives.
Parsing 01,STATIC_DOM_MAX_EVIDENCE_SNIPPETS,120 (env: STATIC_DOM_MAX_EVIDENCE_SNIPPETS),[Backend only — no GUI control] Maximum evidence snippets extracted from static DOM per page. Range 10-500. Controls how many pieces of evidence are harvested from each page's HTML structure.
Parsing 01,DOM_SNIPPET_MAX_CHARS,3600 (env: DOM_SNIPPET_MAX_CHARS),[Backend only — no GUI control] Maximum characters per DOM evidence snippet. Truncates long snippets to prevent oversized evidence entries.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# PARSING — Dynamic JS-Rendered (02),,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Parsing 02,DYNAMIC_CRAWLEE_ENABLED,true (env + GUI: toggle),[GUI: Indexing Runtime → Crawlee Enabled toggle] Same as Phase 05 DYNAMIC_CRAWLEE_ENABLED. Controls Playwright-based dynamic rendering for JS-heavy pages.
Parsing 02,DYNAMIC_FETCH_RETRY_BUDGET,2 (env + GUI: number input),[GUI: Indexing Runtime → Retry Budget input] Same as Phase 05. Retry budget for dynamic page fetches.
Parsing 02,DYNAMIC_FETCH_RETRY_BACKOFF_MS,1200 (env + GUI: number input),[GUI: Indexing Runtime → Retry Backoff input] Same as Phase 05. Backoff between retries.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# PARSING — Article Extraction (03),,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Parsing 03,ARTICLE_EXTRACTOR_V2,true (env: ARTICLE_EXTRACTOR_V2),[Backend only — no GUI control] Enable V2 article extractor using Mozilla Readability + JSDOM. V2 is more accurate than V1 at isolating article content from page chrome.
Parsing 03,ARTICLE_EXTRACTOR_MIN_CHARS,700 (env: ARTICLE_EXTRACTOR_MIN_CHARS),[Backend only — no GUI control] Minimum character count for extracted article content. Articles shorter than 700 chars are rejected as too short to contain useful spec data.
Parsing 03,ARTICLE_EXTRACTOR_MIN_SCORE,45 (env: ARTICLE_EXTRACTOR_MIN_SCORE),[Backend only — no GUI control] Minimum quality score (0-100) for article extraction. The extractor scores articles by readability metrics. Below 45 the content is rejected as too noisy.
Parsing 03,ARTICLE_EXTRACTOR_MAX_CHARS,24000 (env: ARTICLE_EXTRACTOR_MAX_CHARS),[Backend only — no GUI control] Maximum characters to retain from article extraction. Truncates very long articles to keep evidence manageable. 24K chars is ~6 pages of text.
Parsing 03,ARTICLE_EXTRACTOR_DOMAIN_POLICY_MAP_JSON,{} (env: ARTICLE_EXTRACTOR_DOMAIN_POLICY_MAP_JSON),[Backend only — no GUI control] Per-domain article extraction policy overrides as JSON. Allows customizing min score/chars per domain for sites with unusual layouts.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# PARSING — HTML Spec Table (04),,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Parsing 04,HTML_TABLE_EXTRACTOR_V2,true (env: HTML_TABLE_EXTRACTOR_V2),[Backend only — no GUI control] Enable V2 HTML table parser with merged-cell support. V2 correctly handles colspan/rowspan in spec tables that V1 misparses.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# PARSING — Structured Metadata (05),,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Parsing 05,STRUCTURED_METADATA_EXTRUCT_ENABLED,false (env: STRUCTURED_METADATA_EXTRUCT_ENABLED),"[Backend only — no GUI control] Enable structured metadata extraction via the extruct FastAPI sidecar service. When enabled the sidecar extracts JSON-LD, OpenGraph, microdata, and other structured formats. Requires the sidecar to be running."
Parsing 05,STRUCTURED_METADATA_EXTRUCT_URL,http://127.0.0.1:8011/extract/structured (env),[Backend only — no GUI control] URL of the extruct sidecar service. The sidecar is a Python FastAPI server that runs extruct for structured metadata extraction.
Parsing 05,STRUCTURED_METADATA_EXTRUCT_TIMEOUT_MS,2000 (env: STRUCTURED_METADATA_EXTRUCT_TIMEOUT_MS),[Backend only — no GUI control] Timeout for sidecar calls in milliseconds. Range 250-15000ms. 2 seconds is usually sufficient for fast extraction.
Parsing 05,STRUCTURED_METADATA_EXTRUCT_MAX_ITEMS_PER_SURFACE,200 (env),"[Backend only — no GUI control] Maximum structured metadata items per extraction surface (JSON-LD, microdata, etc.). Range 1-1000. Prevents memory bloat from pages with thousands of structured items."
Parsing 05,STRUCTURED_METADATA_EXTRUCT_CACHE_ENABLED,true (env),[Backend only — no GUI control] Enable caching for structured metadata extraction results. Avoids re-parsing the same page content on subsequent runs.
Parsing 05,STRUCTURED_METADATA_EXTRUCT_CACHE_LIMIT,400 (env),[Backend only — no GUI control] Maximum entries in the structured metadata cache. Range 32-5000. Older entries are evicted when the limit is reached.
Parsing 05,Confidence Bases — network_json,0.96 (hardcoded),[Hardcoded — no control] Base confidence for values extracted from intercepted network JSON responses. Highest confidence because network JSON is structured API data directly from the site.
Parsing 05,Confidence Bases — embedded_state,0.93 (hardcoded),[Hardcoded — no control] Base confidence for values from embedded state (window.__INITIAL_STATE__ etc.). Very high confidence as it's pre-rendered app state.
Parsing 05,Confidence Bases — json_ld,0.90 (hardcoded),[Hardcoded — no control] Base confidence for JSON-LD structured data. High confidence because JSON-LD follows standardized schemas.
Parsing 05,Confidence Bases — microdata,0.88 (hardcoded),[Hardcoded — no control] Base confidence for HTML Microdata. Slightly lower than JSON-LD because microdata is embedded in HTML attributes which are more error-prone.
Parsing 05,Confidence Bases — opengraph,0.80 (hardcoded),[Hardcoded — no control] Base confidence for OpenGraph metadata. Moderate confidence — OG tags are marketing-oriented and may contain simplified/inaccurate specs.
Parsing 05,Confidence Bases — microformat_rdfa,0.78 (hardcoded),[Hardcoded — no control] Base confidence for Microformats and RDFa. Lowest structured metadata confidence as these formats are less commonly used for technical specs.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# PARSING — PDF (06),,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Parsing 06,PDF_BACKEND_ROUTER_ENABLED,false (env: PDF_BACKEND_ROUTER_ENABLED),"[Backend only — no GUI control] Enable the multi-backend PDF parser. When enabled the router tries multiple PDF extraction backends (pdfplumber, pymupdf, camelot, tabula) and picks the best result. When disabled only the legacy pdf-parse library is used."
Parsing 06,PDF_PREFERRED_BACKEND,auto (env: PDF_PREFERRED_BACKEND),[Backend only — no GUI control] Preferred PDF backend when router is enabled. 'auto' tries all backends and picks the best result. Can be set to a specific backend to force its use.
Parsing 06,PDF_BACKEND_ROUTER_TIMEOUT_MS,120000 (env: PDF_BACKEND_ROUTER_TIMEOUT_MS),[Backend only — no GUI control] Timeout for PDF backend operations in milliseconds. Range 10000-300000 (10s-5min). 2 minutes allows for large PDFs with complex layouts.
Parsing 06,PDF_BACKEND_ROUTER_MAX_PAGES,60 (env: PDF_BACKEND_ROUTER_MAX_PAGES),[Backend only — no GUI control] Maximum PDF pages to process. Range 1-300. Most spec PDFs are under 60 pages. Prevents processing entire 300-page product manuals.
Parsing 06,PDF_BACKEND_ROUTER_MAX_PAIRS,5000 (env: PDF_BACKEND_ROUTER_MAX_PAIRS),[Backend only — no GUI control] Maximum field-value pairs to extract from PDF. Range 100-20000. Limits the data volume from very large spec PDFs.
Parsing 06,PDF_BACKEND_ROUTER_MAX_TEXT_PREVIEW_CHARS,20000 (env: PDF_BACKEND_ROUTER_MAX_TEXT_PREVIEW_CHARS),[Backend only — no GUI control] Maximum characters in the PDF text preview. Range 1000-100000. The preview is used for quick content assessment before full extraction.
Parsing 06,MAX_PDF_BYTES,8000000 (env: MAX_PDF_BYTES),[Backend only — no GUI control] Maximum PDF file size in bytes (~8MB). PDFs larger than this are skipped entirely. Prevents downloading huge files that would take too long to process.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# PARSING — Scanned PDF OCR (07),,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Parsing 07,SCANNED_PDF_OCR_ENABLED,true (env + GUI: Indexing → OCR Enabled toggle),[GUI: Indexing Runtime → Scanned PDF OCR Enabled toggle] Master switch for OCR processing of scanned PDFs. When enabled PDFs detected as scanned images are processed through OCR (Tesseract) to extract text.
Parsing 07,SCANNED_PDF_OCR_PROMOTE_CANDIDATES,true (env + GUI: Indexing → Promote OCR toggle),[GUI: Indexing Runtime → Promote OCR Candidates toggle] When enabled OCR-derived values are promoted to extraction candidates alongside deterministic parsing results. When disabled OCR text is available but not auto-promoted.
Parsing 07,SCANNED_PDF_OCR_BACKEND,auto (env + GUI: Indexing → OCR Backend dropdown: auto/tesseract/none),[GUI: Indexing Runtime → OCR Backend dropdown] OCR engine selection. 'auto' tries available backends. 'tesseract' forces Tesseract OCR. 'none' disables OCR processing.
Parsing 07,SCANNED_PDF_OCR_MAX_PAGES,4 (env + GUI: Indexing → OCR Max Pages input),"[GUI: Indexing Runtime → OCR Max Pages input (min 1, max 100)] Maximum pages to OCR from a scanned PDF. Default 4 processes just the first few pages where spec tables usually appear."
Parsing 07,SCANNED_PDF_OCR_MAX_PAIRS,800 (env + GUI: Indexing → OCR Max Pairs input),"[GUI: Indexing Runtime → OCR Max Pairs input (min 50, max 20000)] Maximum key-value pairs to extract from OCR text. Controls extraction volume from OCR results."
Parsing 07,SCANNED_PDF_OCR_MIN_CHARS_PER_PAGE,30 (env + GUI: Indexing → Min Chars/Page input),"[GUI: Indexing Runtime → Min Chars/Page input (min 1, max 500)] Minimum OCR characters per page to accept the page as valid. Pages with fewer than 30 recognized characters are rejected as blank or failed OCR."
Parsing 07,SCANNED_PDF_OCR_MIN_LINES_PER_PAGE,2 (env + GUI: Indexing → Min Lines/Page input),"[GUI: Indexing Runtime → Min Lines/Page input (min 1, max 100)] Minimum OCR lines per page to accept. Pages with fewer than 2 recognized lines are rejected."
Parsing 07,SCANNED_PDF_OCR_MIN_CONFIDENCE,0.50 (env + GUI: Indexing → Min Confidence input),"[GUI: Indexing Runtime → Min Confidence input (min 0, max 1, step 0.01)] Minimum OCR confidence threshold. Characters/words recognized with below 50% confidence are rejected. Higher values are stricter but may miss valid text from low-quality scans."
# ═══════════════════════════════════════════════════════════════════════════════,,,
# PARSING — Chart/Graph Extraction (09),,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Parsing 09,CHART_EXTRACTION_ENABLED,true (planned; partially implemented),[Backend only — no GUI control] Master switch for chart and graph extraction from page content. Currently only network payload interception is implemented. Full SVG parser and chart detector are pending.
Parsing 09,CHART_VISION_FALLBACK_ENABLED,false (planned),[NOT IMPLEMENTED] Planned switch to enable vision LLM fallback for raster charts that can't be parsed structurally. Would send chart images to a vision model for numeric extraction.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# EVIDENCE PACK,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Evidence,Evidence Text Max Chars,5000 (hardcoded in evidencePackV2.js),[Hardcoded — no control] Maximum characters for evidence text conversion. Truncates evidence values longer than 5000 chars when converting to text format for LLM consumption.
Evidence,Evidence Headings Limit,120 (hardcoded in evidencePackV2.js),[Hardcoded — no control] Maximum heading elements retained from evidence source. Limits how many document headings are included in the evidence pack.
Evidence,Evidence Chunk Max Length,3000 (hardcoded in evidencePackV2.js),[Hardcoded — no control] Maximum length of individual evidence chunks. Long evidence is broken into chunks of up to 3000 chars.
Evidence,Evidence Spec Sections Limit,8 (hardcoded in evidencePackV2.js),[Hardcoded — no control] Maximum spec table sections retained per evidence source. Limits how many distinct spec tables are included from a single page.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# LLM BUDGET & COST CONTROLS,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
LLM Budget,LLM_ENABLED,false (env: LLM_ENABLED),"[Backend only — no GUI control] Master switch for all LLM functionality. When false no LLM calls are made — only deterministic extraction. Must be enabled for LLM-based extraction, triage, and planning."
LLM Budget,LLM_MONTHLY_BUDGET_USD,200 (env: LLM_MONTHLY_BUDGET_USD),[Backend only — no GUI control] Monthly spending cap for LLM API calls in USD. When cumulative monthly spend reaches $200 all LLM calls are blocked. Prevents runaway costs.
LLM Budget,LLM_PER_PRODUCT_BUDGET_USD,0.10 (env: LLM_PER_PRODUCT_BUDGET_USD),[Backend only — no GUI control] Per-product spending cap in USD. After $0.10 spent on a single product no more LLM calls are made for it. Prevents a single difficult product from consuming the entire budget.
LLM Budget,LLM_DISABLE_BUDGET_GUARDS,false (env: LLM_DISABLE_BUDGET_GUARDS),[Backend only — no GUI control] Disable all budget enforcement. When true there are no spending limits. Use with caution in production.
LLM Budget,LLM_MAX_BATCHES_PER_PRODUCT,7 (env: LLM_MAX_BATCHES_PER_PRODUCT),[Backend only — no GUI control] Maximum extraction batches per product. Each batch is a group of fields sent to the LLM together. More batches = more comprehensive extraction but higher cost.
LLM Budget,LLM_MAX_CALLS_PER_PRODUCT_TOTAL,10 (env: LLM_MAX_CALLS_PER_PRODUCT_TOTAL),"[Backend only — no GUI control] Hard cap on total LLM API calls per product across all roles (extract, validate, write, triage). Absolute backstop against runaway calls."
LLM Budget,LLM_MAX_CALLS_PER_PRODUCT_FAST,2 (env: LLM_MAX_CALLS_PER_PRODUCT_FAST),[Backend only — no GUI control] Maximum fast-pass LLM calls per product. Limits the number of cheap/quick LLM calls for triage and classification.
LLM Budget,LLM_MAX_CALLS_PER_ROUND,4 (env: LLM_MAX_CALLS_PER_ROUND),[Backend only — no GUI control] Maximum LLM calls per convergence round. Distributes the call budget across rounds rather than spending it all in round 1.
LLM Budget,LLM_MAX_EVIDENCE_CHARS,60000 (env: LLM_MAX_EVIDENCE_CHARS),[Backend only — no GUI control] Maximum total characters of evidence sent to the LLM per extraction call. Limits context window usage. 60K chars is roughly 15-20K tokens.
LLM Budget,LLM_MAX_TOKENS,16384 (env: LLM_MAX_TOKENS),[Backend only — no GUI control] General max tokens for LLM calls. Acts as a safety cap across all roles.
LLM Budget,LLM_MAX_OUTPUT_TOKENS,1200 (env: LLM_MAX_OUTPUT_TOKENS),[Backend only — no GUI control] General max output tokens. Default for any role that doesn't have its own specific setting.
LLM Budget,LLM_TIMEOUT_MS,40000 (env: LLM_TIMEOUT_MS),[Backend only — no GUI control] LLM API request timeout in milliseconds. 40 seconds allows for slow model responses. Increase for reasoning models; decrease for fast models.
LLM Budget,LLM_EXTRACTION_CACHE_ENABLED,true (env: LLM_EXTRACTION_CACHE_ENABLED),[Backend only — no GUI control] Enable disk caching for LLM extraction results. Avoids re-extracting the same evidence on re-runs. Saves significant cost during development and re-runs.
LLM Budget,LLM_EXTRACTION_CACHE_TTL_MS,604800000 / 7 days (env: LLM_EXTRACTION_CACHE_TTL_MS),[Backend only — no GUI control] Time-to-live for cached LLM extraction results in milliseconds. Cached results older than 7 days are discarded. Balances freshness with cost savings.
LLM Budget,LLM_EXTRACTION_CACHE_DIR,.specfactory_tmp/llm_cache (env),[Backend only — no GUI control] Directory for LLM extraction cache files. Each cached response is a JSON file keyed by content hash.
LLM Budget,LLM_COST_INPUT_PER_1M,1.25 (env: LLM_COST_INPUT_PER_1M),[Backend only — no GUI control] Default cost per 1 million input tokens in USD. Used for budget tracking when model-specific pricing isn't available.
LLM Budget,LLM_COST_OUTPUT_PER_1M,10 (env: LLM_COST_OUTPUT_PER_1M),[Backend only — no GUI control] Default cost per 1 million output tokens in USD. Output tokens are typically 8× more expensive than input.
LLM Budget,LLM_COST_CACHED_INPUT_PER_1M,0.125 (env: LLM_COST_CACHED_INPUT_PER_1M),[Backend only — no GUI control] Default cost per 1M cached input tokens. Cached inputs are 10× cheaper than fresh inputs for supported providers.
LLM Budget,LLM_VERIFY_MODE,false (env: LLM_VERIFY_MODE),[Backend only — no GUI control] Enable verification mode where the LLM double-checks extracted values. Increases accuracy but doubles LLM costs.
LLM Budget,LLM_VERIFY_SAMPLE_RATE,10 (env: LLM_VERIFY_SAMPLE_RATE),[Backend only — no GUI control] Percentage of extractions to verify when verify mode is enabled. 10% means 1 in 10 extractions get a verification pass. Lower values save costs.
LLM Budget,LLM_WRITE_SUMMARY,false (env: LLM_WRITE_SUMMARY),[Backend only — no GUI control] Use LLM to generate a human-readable product summary after extraction. Additional cost per product for a written paragraph summarizing key specs.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# LLM SETTINGS PAGE — Per-Route Controls,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
LLM Routes,Required Level (per route),(GUI: LLM Settings → Route dropdown),[GUI: LLM Settings → Per-Route → Required Level dropdown] Sets the required level for each field route: identity / required / critical / expected / optional / editorial / commerce. Controls how aggressively the pipeline pursues evidence for this field.
LLM Routes,Availability (per route),(GUI: LLM Settings → Route dropdown),[GUI: LLM Settings → Per-Route → Availability dropdown] Expected availability of this field: always / expected / sometimes / rare / editorial_only. Informs the convergence loop whether to keep searching or give up.
LLM Routes,Difficulty (per route),(GUI: LLM Settings → Route dropdown),[GUI: LLM Settings → Per-Route → Difficulty dropdown] Extraction difficulty: easy / medium / hard / instrumented. Hard fields get more LLM budget and reasoning model escalation.
LLM Routes,Effort (per route),(GUI: LLM Settings → Effort slider 1-10),"[GUI: LLM Settings → Per-Route → Effort slider] Effort level 1-10 controlling how much resource budget is allocated to extracting this field. Higher effort = more search queries, more evidence gathering, more LLM calls."
LLM Routes,Model Ladder,(GUI: LLM Settings → text input),[GUI: LLM Settings → Per-Route → Model Ladder input] Text input specifying the model escalation ladder for this route. Defines which models are tried in order (fast → balanced → deep) for this field.
LLM Routes,Max Tokens (per route),(GUI: LLM Settings → slider 1024-32768),[GUI: LLM Settings → Per-Route → Max Tokens slider] Maximum output tokens for LLM calls on this specific field route. Allows per-field token budgets.
LLM Routes,Min Evidence Refs (per route),(GUI: LLM Settings → slider 1-5),[GUI: LLM Settings → Per-Route → Min Evidence Refs slider] Minimum evidence references required before accepting this field's value. Higher values require more corroborating sources.
LLM Routes,Insufficient Evidence Action,(GUI: LLM Settings → dropdown),[GUI: LLM Settings → Per-Route → Insufficient Evidence Action dropdown] What to do when min evidence refs aren't met: threshold_unmet (mark as unmet) / return_unk (return unknown) / escalate (try harder with deeper model).
LLM Routes,Context Pack,(GUI: LLM Settings → dropdown),[GUI: LLM Settings → Per-Route → Context Pack dropdown] How much context to send to the LLM: standard / minimal / full. Controls the evidence context window size for this route.
LLM Routes,single_source_data,(GUI: LLM Settings → toggle),[GUI: LLM Settings → Per-Route → Source Package → Single Source toggle] Send data from a single best source to the LLM. When true only the top-ranked source is sent.
LLM Routes,all_source_data,(GUI: LLM Settings → toggle),[GUI: LLM Settings → Per-Route → Source Package → All Sources toggle] Send data from all available sources to the LLM. When true all evidence is sent for cross-reference.
LLM Routes,enable_websearch,(GUI: LLM Settings → toggle),[GUI: LLM Settings → Per-Route → Source Package → Web Search toggle] Enable web search for this specific route. Can be disabled per-field to save search budget.
LLM Routes,all_sources_confidence_repatch,(GUI: LLM Settings → toggle),[GUI: LLM Settings → Per-Route → Source Package → Confidence Repatch toggle] Re-calculate confidence after seeing all sources. Useful when initial extraction was from a single source.
LLM Routes,Scalar/Component/List Send Mode,(GUI: LLM Settings → dropdowns),"[GUI: LLM Settings → Per-Route → Send Mode dropdowns] Controls what data is sent alongside the field value in extraction: value only vs value + prime sources. Separate dropdowns for scalar fields, component fields, and list fields."
LLM Routes,14 Advanced Prompt Flags,(GUI: LLM Settings → 14 toggle checkboxes),"[GUI: LLM Settings → Per-Route → Advanced Prompt Flags] 14 boolean flags controlling which studio metadata is included in the extraction prompt: key navigation, contract rules, extraction guidance, tooltips, enum options, component constraints, parse templates, AI mode difficulty, required level, entity sets, evidence policy, variance policy, constraints, boolean prompts. Each flag adds/removes a section from the LLM prompt."
# ═══════════════════════════════════════════════════════════════════════════════,,,
# DISCOVERY & CRAWLING,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Discovery,DISCOVERY_ENABLED,false (env + GUI: Indexing → Provider Discovery toggle),[GUI: Indexing Runtime → Provider Discovery toggle] Master switch for web search discovery. When enabled the pipeline searches the web for new source URLs. When disabled only pre-existing seed URLs are used.
Discovery,RUN_PROFILE,standard (env + GUI: Indexing → Run Profile dropdown: fast/standard/thorough),[GUI: Indexing Runtime → Run Profile dropdown] Run profile controlling overall aggressiveness. 'fast' reduces queries/URLs/concurrency for quick runs. 'thorough' increases everything for comprehensive extraction. 'standard' is balanced.
Discovery,DISCOVERY_MAX_QUERIES,6 (env: DISCOVERY_MAX_QUERIES),"[Backend only — no GUI control] Maximum search queries per discovery pass. Adjusted by run profile: thorough=24, standard=6, fast=4. Controls the search budget per pass."
Discovery,DISCOVERY_RESULTS_PER_QUERY,10 (env: DISCOVERY_RESULTS_PER_QUERY),"[Backend only — no GUI control] Results per search query. Adjusted by run profile: thorough=20, standard=10, fast=6. Controls how many SERP results are considered per query."
Discovery,DISCOVERY_MAX_DISCOVERED,80 (env: DISCOVERY_MAX_DISCOVERED),"[Backend only — no GUI control] Maximum total discovered URLs. Adjusted by run profile: thorough=300, standard=80, fast=60. Hard cap on total URLs found via search."
Discovery,DISCOVERY_QUERY_CONCURRENCY,4 (env: DISCOVERY_QUERY_CONCURRENCY),"[Backend only — no GUI control] Concurrent search queries during discovery. Adjusted by run profile: thorough=8, standard=4. Controls how many queries run in parallel."
Discovery,MAX_URLS_PER_PRODUCT,20 (env: MAX_URLS_PER_PRODUCT),[Backend only — no GUI control] Maximum URLs fetched per product across all rounds. Hard cap on total pages downloaded.
Discovery,MAX_CANDIDATE_URLS,50 (env: MAX_CANDIDATE_URLS_PER_PRODUCT or MAX_CANDIDATE_URLS),[Backend only — no GUI control] Maximum candidate URLs tracked per product. Candidates are URLs found but not yet fetched. Larger pool means more options for the fetch scheduler.
Discovery,MAX_PAGES_PER_DOMAIN,2 (env: MAX_PAGES_PER_DOMAIN),[Backend only — no GUI control] Maximum pages fetched per domain per product. Prevents over-crawling any single site. 2 pages usually gets the spec page + one related page.
Discovery,MAX_RUN_SECONDS,300 (env: MAX_RUN_SECONDS),[Backend only — no GUI control] Maximum total runtime per product in seconds. 5-minute hard stop prevents runaway processing.
Discovery,MANUFACTURER_DEEP_RESEARCH_ENABLED,true (env),"[Backend only — no GUI control] Enable deep research mode for manufacturer URLs. When enabled the system explores manufacturer sites more thoroughly — following links to spec subpages, PDF downloads, and support docs."
Discovery,MAX_MANUFACTURER_URLS_PER_PRODUCT,20 (env),[Backend only — no GUI control] Maximum manufacturer URLs per product. Allows deep crawling of manufacturer sites beyond the normal MAX_PAGES_PER_DOMAIN limit.
Discovery,MAX_MANUFACTURER_PAGES_PER_DOMAIN,8 (env),[Backend only — no GUI control] Maximum pages per manufacturer domain. Higher than the general limit to allow thorough manufacturer exploration.
Discovery,MANUFACTURER_RESERVE_URLS,10 (env),[Backend only — no GUI control] URL slots reserved for manufacturer discovery. Ensures manufacturer sources always have room in the fetch budget.
Discovery,MAX_JSON_BYTES,2000000 (env: MAX_JSON_BYTES),[Backend only — no GUI control] Maximum JSON response size in bytes (~2MB). Prevents memory issues from enormous API responses.
Discovery,USER_AGENT,Mozilla/5.0 (compatible; EGSpecHarvester/1.0) (env),[Backend only — no GUI control] HTTP User-Agent header sent with all requests. Identifies the crawler to web servers.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# AGGRESSIVE MODE,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Aggressive,AGGRESSIVE_MODE_ENABLED,false (env: AGGRESSIVE_MODE_ENABLED),"[Backend only — no GUI control] Enable aggressive extraction mode with higher limits across all phases. Significantly increases search budget, LLM calls, and time per product. Much more expensive but more thorough."
Aggressive,AGGRESSIVE_CONFIDENCE_THRESHOLD,0.85 (env),[Backend only — no GUI control] Confidence threshold for aggressive mode field acceptance. Stricter than default to compensate for the larger evidence pool.
Aggressive,AGGRESSIVE_MAX_SEARCH_QUERIES,5 (env),[Backend only — no GUI control] Max search queries in aggressive mode per pass.
Aggressive,AGGRESSIVE_EVIDENCE_AUDIT_ENABLED,true (env),[Backend only — no GUI control] Enable evidence audit in aggressive mode. Cross-checks extracted values against evidence for consistency.
Aggressive,AGGRESSIVE_EVIDENCE_AUDIT_BATCH_SIZE,60 (env),[Backend only — no GUI control] Batch size for evidence auditing. 60 evidence items per audit batch.
Aggressive,AGGRESSIVE_MAX_TIME_PER_PRODUCT_MS,600000 (env),[Backend only — no GUI control] Max time per product in aggressive mode: 10 minutes (2× standard). Allows more time for thorough extraction.
Aggressive,AGGRESSIVE_THOROUGH_FROM_ROUND,2 (env),[Backend only — no GUI control] Round number at which aggressive mode switches to thorough search behavior. Round 1 uses standard settings; round 2+ enables deep search.
Aggressive,AGGRESSIVE_ROUND1_MAX_URLS,90 (env),[Backend only — no GUI control] Max URLs in round 1 of aggressive mode. Much higher than standard (20).
Aggressive,AGGRESSIVE_ROUND1_MAX_CANDIDATE_URLS,120 (env),[Backend only — no GUI control] Max candidate URLs in round 1 of aggressive mode.
Aggressive,AGGRESSIVE_LLM_MAX_CALLS_PER_ROUND,16 (env),[Backend only — no GUI control] Max LLM calls per round in aggressive mode. 4× the standard limit.
Aggressive,AGGRESSIVE_LLM_MAX_CALLS_PER_PRODUCT_TOTAL,48 (env),[Backend only — no GUI control] Max total LLM calls per product in aggressive mode. Nearly 5× the standard limit.
Aggressive,AGGRESSIVE_LLM_TARGET_MAX_FIELDS,75 (env),[Backend only — no GUI control] Target max fields to fill in aggressive mode. Tries to fill up to 75 fields per product.
Aggressive,AGGRESSIVE_LLM_DISCOVERY_PASSES,3 (env),[Backend only — no GUI control] Number of discovery passes in aggressive mode. Each pass generates new search queries based on what's still missing.
Aggressive,AGGRESSIVE_LLM_DISCOVERY_QUERY_CAP,24 (env),[Backend only — no GUI control] Query cap per discovery pass in aggressive mode.
Aggressive,UBER_AGGRESSIVE_ENABLED,false (env: UBER_AGGRESSIVE_ENABLED),[Backend only — no GUI control] Enable uber-aggressive mode — the most thorough extraction profile. Even higher limits than aggressive mode.
Aggressive,UBER_MAX_URLS_PER_PRODUCT,25 (env),[Backend only — no GUI control] Max URLs per product in uber-aggressive mode.
Aggressive,UBER_MAX_URLS_PER_DOMAIN,6 (env),[Backend only — no GUI control] Max URLs per domain in uber-aggressive mode. 3× the standard limit.
Aggressive,UBER_MAX_ROUNDS,6 (env),[Backend only — no GUI control] Max convergence rounds in uber-aggressive mode. 2× the standard limit.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# CORTEX (External LLM Orchestrator),,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
CORTEX,CORTEX_ENABLED,false (env: CORTEX_ENABLED),"[Backend only — no GUI control] Enable CORTEX external LLM orchestrator. CORTEX provides advanced model routing, escalation, and async processing. Requires a separate CORTEX server."
CORTEX,CORTEX_BASE_URL,http://localhost:5001/v1 (env),[Backend only — no GUI control] CORTEX sync API base URL.
CORTEX,CORTEX_API_KEY,key (env: CORTEX_API_KEY),[Backend only — no GUI control] CORTEX API authentication key.
CORTEX,CORTEX_ASYNC_ENABLED,true (env),[Backend only — no GUI control] Enable CORTEX async mode for long-running LLM jobs. Jobs are submitted and polled for results.
CORTEX,CORTEX_SYNC_TIMEOUT_MS,60000 (env),[Backend only — no GUI control] Timeout for synchronous CORTEX calls.
CORTEX,CORTEX_ASYNC_POLL_INTERVAL_MS,5000 (env),[Backend only — no GUI control] Polling interval for async CORTEX jobs.
CORTEX,CORTEX_ASYNC_MAX_WAIT_MS,900000 (env),[Backend only — no GUI control] Maximum wait time for async CORTEX jobs: 15 minutes.
CORTEX,CORTEX_ESCALATE_CONFIDENCE_LT,0.85 (env),[Backend only — no GUI control] Confidence threshold below which CORTEX escalates to a deeper model.
CORTEX,CORTEX_ESCALATE_IF_CONFLICT,true (env),[Backend only — no GUI control] Escalate to deeper CORTEX model when field conflicts are detected.
CORTEX,CORTEX_ESCALATE_CRITICAL_ONLY,true (env),[Backend only — no GUI control] Only escalate critical/identity fields — not optional ones.
CORTEX,CORTEX_MAX_DEEP_FIELDS_PER_PRODUCT,12 (env),[Backend only — no GUI control] Maximum fields per product that get deep model escalation via CORTEX.
CORTEX,CORTEX_FAILURE_THRESHOLD,3 (env),[Backend only — no GUI control] Number of consecutive CORTEX failures before circuit breaker opens.
CORTEX,CORTEX_CIRCUIT_OPEN_MS,30000 (env),[Backend only — no GUI control] Duration the circuit breaker stays open after CORTEX failures: 30 seconds.
CORTEX,CORTEX Model Selection (6 models),fast: gpt-5-low / reasoning: gpt-5-high / plus audit/dom/search/rerank/vision variants,"[Backend only — no GUI control] Per-role model selection for CORTEX. 6 model slots: fast, audit, dom, reasoning_deep, vision, search_fast, rerank_fast, search_deep. Each can be set independently."
# ═══════════════════════════════════════════════════════════════════════════════,,,
# INDEXING RESUME & REEXTRACT,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Indexing,INDEXING_RESUME_MODE,auto (env: INDEXING_RESUME_MODE),[Backend only — no GUI control] Resume mode for interrupted indexing runs. 'auto' resumes if a recent run exists. 'manual' requires explicit resume flag. 'off' always starts fresh.
Indexing,INDEXING_RESUME_MAX_AGE_HOURS,48 (env),[Backend only — no GUI control] Maximum age in hours for a run to be eligible for resume. Runs older than 48 hours are considered stale and not resumed.
Indexing,INDEXING_RESUME_SEED_LIMIT,24 (env),[Backend only — no GUI control] Maximum seed URLs loaded during resume. Limits how many URLs are re-seeded from the previous run.
Indexing,INDEXING_RESUME_PERSIST_LIMIT,160 (env),[Backend only — no GUI control] Maximum persisted items loaded during resume.
Indexing,INDEXING_REEXTRACT_ENABLED,true (env),[Backend only — no GUI control] Enable re-extraction for stale evidence. When enabled previously-extracted pages are re-processed if they exceed the staleness threshold.
Indexing,INDEXING_REEXTRACT_AFTER_HOURS,24 (env),[Backend only — no GUI control] Hours after which extracted evidence becomes stale and eligible for re-extraction.
Indexing,INDEXING_SCHEMA_PACKETS_VALIDATION_ENABLED,true (env),[Backend only — no GUI control] Enable JSON schema validation for indexing packets. When enabled all event payloads are validated against their schemas.
Indexing,INDEXING_SCHEMA_PACKETS_VALIDATION_STRICT,true (env),[Backend only — no GUI control] Strict schema validation mode. When true validation failures throw errors. When false they log warnings.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# SOURCE STRATEGY (Pipeline Settings GUI),,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Source Strategy,Source Strategy Table,(GUI: Pipeline Settings → Source Strategy table),[GUI: Pipeline Settings → Source Strategy table] Configurable table of source strategies with per-row enable/disable toggles and delete buttons. Each row defines a source discovery strategy with its parameters. Strategies control how the pipeline finds new URLs.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# DRIFT DETECTION & MAINTENANCE,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Drift,DRIFT_DETECTION_ENABLED,true (env: DRIFT_DETECTION_ENABLED),[Backend only — no GUI control] Enable drift detection for published specs. Periodically re-checks published product specs for changes. Detects when source pages update their spec values.
Drift,DRIFT_POLL_SECONDS,86400 / 24 hours (env),[Backend only — no GUI control] Polling interval for drift detection in seconds. Checks for spec drift once per day by default.
Drift,DRIFT_SCAN_MAX_PRODUCTS,250 (env),[Backend only — no GUI control] Maximum products to scan per drift detection cycle.
Drift,DRIFT_AUTO_REPUBLISH,true (env),[Backend only — no GUI control] Automatically republish specs when drift is detected. When true changed values are automatically updated.
Drift,RECRAWL_STALE_AFTER_DAYS,30 (env),[Backend only — no GUI control] Re-crawl product source URLs after this many days. 30-day staleness window ensures evidence stays fresh.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# DAEMON & IMPORTS,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Daemon,DAEMON_CONCURRENCY,3 (env: DAEMON_CONCURRENCY),[Backend only — no GUI control] Number of concurrent product runs in daemon mode. Controls how many products the background service processes simultaneously.
Daemon,DAEMON_GRACEFUL_SHUTDOWN_TIMEOUT_MS,60000 (env),[Backend only — no GUI control] Grace period for daemon shutdown in milliseconds. Allows 60 seconds for in-progress runs to complete before force-stopping.
Daemon,IMPORTS_ROOT,imports (env: IMPORTS_ROOT),[Backend only — no GUI control] Root directory for the import watcher. The daemon monitors this directory for new product import files.
Daemon,IMPORTS_POLL_SECONDS,10 (env: IMPORTS_POLL_SECONDS),[Backend only — no GUI control] Polling interval for the import watcher in seconds. Checks for new import files every 10 seconds.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# HYPOTHESIS & SELF-IMPROVEMENT,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Hypothesis,SELF_IMPROVE_ENABLED,true (env: SELF_IMPROVE_ENABLED),[Backend only — no GUI control] Enable self-improvement hypothesis system. The pipeline generates hypotheses about where to find missing specs and tests them in follow-up rounds.
Hypothesis,MAX_HYPOTHESIS_ITEMS,50 (env: MAX_HYPOTHESIS_ITEMS),[Backend only — no GUI control] Maximum hypothesis items tracked per product. Limits the hypothesis pool size.
Hypothesis,HYPOTHESIS_AUTO_FOLLOWUP_ROUNDS,0 (env),[Backend only — no GUI control] Number of automatic follow-up rounds for hypothesis testing. 0 means no automatic follow-up — hypotheses are tested only in scheduled rounds.
Hypothesis,HYPOTHESIS_FOLLOWUP_URLS_PER_ROUND,12 (env),[Backend only — no GUI control] Maximum URLs fetched per hypothesis follow-up round.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# OUTPUT & STORAGE,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Output,OUTPUT_MODE,dual (env: OUTPUT_MODE),"[Backend only — no GUI control] Output storage mode: 'local' (disk only), 's3' (S3 only), 'dual' (both). Controls where completed product specs are saved."
Output,LOCAL_MODE,false (env: LOCAL_MODE),[Backend only — no GUI control] Run entirely locally with no cloud dependencies. Disables S3 mirroring and remote features.
Output,DRY_RUN,false (env: DRY_RUN),[Backend only — no GUI control] Dry run mode — no files are written. Useful for testing pipeline behavior without producing output.
Output,LOCAL_INPUT_ROOT,fixtures/s3 (env: LOCAL_INPUT_ROOT or LOCAL_S3_ROOT),[Backend only — no GUI control] Root directory for local input fixtures. Contains product seed files organized by category.
Output,LOCAL_OUTPUT_ROOT,out (env: LOCAL_OUTPUT_ROOT),[Backend only — no GUI control] Root directory for pipeline output files. Completed specs are written here.
Output,WRITE_MARKDOWN_SUMMARY,true (env: WRITE_MARKDOWN_SUMMARY),[Backend only — no GUI control] Write a markdown summary file alongside JSON output. Human-readable summary of extracted specs.
# ═══════════════════════════════════════════════════════════════════════════════,,,
# SQLITE DUAL-WRITE FLAGS,,,
# ═══════════════════════════════════════════════════════════════════════════════,,,
Dual-Write,QUEUE_JSON_WRITE,false (env),[Backend only — no GUI control] Dual-write queue data to both SQLite and JSON. Migration flag — when true both storage backends receive writes. Used during SQLite migration for safety.
Dual-Write,BILLING_JSON_WRITE,false (env),[Backend only — no GUI control] Dual-write billing data to JSON alongside SQLite.
Dual-Write,BRAIN_JSON_WRITE,false (env),[Backend only — no GUI control] Dual-write brain/knowledge data to JSON.
Dual-Write,INTEL_JSON_WRITE,false (env),[Backend only — no GUI control] Dual-write intel/discovery data to JSON.
Dual-Write,CORPUS_JSON_WRITE,false (env),[Backend only — no GUI control] Dual-write corpus/evidence data to JSON.
Dual-Write,LEARNING_JSON_WRITE,false (env),[Backend only — no GUI control] Dual-write learning store data to JSON.
Dual-Write,CACHE_JSON_WRITE,false (env),[Backend only — no GUI control] Dual-write cache data to JSON.
